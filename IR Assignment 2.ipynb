{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "import xml.dom.minidom\n",
    "import numpy as np\n",
    "import math\n",
    "#from sets import Set\n",
    "#from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(mode):\n",
    "    \"\"\"\n",
    "    It takes only path of folder, no file name.\n",
    "    It only returns the folder which contain all the text file.\n",
    "    \n",
    "    Argument:\n",
    "    #nothing\n",
    "    \n",
    "    Returns:\n",
    "    dp -- directory path which contains all the txt files.\n",
    "    \"\"\"\n",
    "    if (mode == \"input\"):   \n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input\"\n",
    "    elif (mode == \"output\"):\n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/out\"\n",
    "    else:\n",
    "        raise ValueError('Unspecified mode for I/O.')\n",
    "        dp = None\n",
    "\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : read_stop_list\n",
    "def read_text_in_list_form(file_path):\n",
    "    \"\"\"\n",
    "    This function takes the path of stop words file and reads it and returns a list of words.\n",
    "    \n",
    "    Argument:\n",
    "    stop_file_path -- path should be like: path + file name.extension\n",
    "        \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\".\n",
    "    \n",
    "    Returns:\n",
    "    lineList -- list of words containg all the stop_words.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = [line.rstrip('\\n') for line in open(file_path)]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/stoplist.txt\"\n",
    "stop_words = read_text_in_list_form(stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : To open term_ids with terms\n",
    "def open_inverted_index(file_name):\n",
    "    tokensDict = dict()\n",
    "    i = 0\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    for value in file:\n",
    "        value = value.split(\"/\")\n",
    "        value[1] = value[1].strip(\"\\n\")\n",
    "        tokensDict[str(value[1])] = str(value[0])\n",
    "        i = i + 1\n",
    "    file.close()\n",
    "    return tokensDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xml_parser(file_name):\n",
    "#     doc = xml.dom.minidom.parse(file_name)\n",
    "#     qrys = doc.getElementsByTagName('query')\n",
    "#     tpcs = doc.getElementsByTagName('topic')\n",
    "#     queries_list = list()\n",
    "#     #qrys = mydoc.getElementsByTagName('topic')\n",
    "#     i = 0\n",
    "#     for elem in qrys:\n",
    "#         #print(tpcs[i].attributes['number'].value)\n",
    "#         queries_list.append(elem.firstChild.data)\n",
    "#         i = i + 1\n",
    "#         #print(elem.firstChild.data)\n",
    "#     #print(queries_list)\n",
    "\n",
    "## Version 2.0 using dictionary\n",
    "def xml_parser(file_name):\n",
    "    doc = xml.dom.minidom.parse(file_name)\n",
    "    qrys = doc.getElementsByTagName('query')\n",
    "    tpcs = doc.getElementsByTagName('topic')\n",
    "    queries = dict()\n",
    "    #qrys = mydoc.getElementsByTagName('topic')\n",
    "    i = 0\n",
    "    for elem in qrys:\n",
    "        queries[tpcs[i].attributes['number'].value] = elem.firstChild.data\n",
    "        #queries_list.append(elem.firstChild.data)\n",
    "        i = i + 1\n",
    "        #print(elem.firstChild.data)\n",
    "    #print(queries)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'202': 'uss carl vinson', '214': 'capital gains tax rate', '216': 'nicolas cage movies', '221': 'electoral college 2008 results', '227': 'i will survive lyrics', '230': \"world's biggest dog\", '234': 'dark chocolate health benefits', '243': 'afghanistan flag', '246': 'civil war battles in South Carolina', '250': 'ford edge problems'}\n"
     ]
    }
   ],
   "source": [
    "file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "qrys = xml_parser(file_name)\n",
    "#print(qrys[str(202)])\n",
    "print(qrys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_relevance_judgements(file_name):\n",
    "#     file = open(file_name,'r',encoding = \"utf-8\")\n",
    "#     for line in file:\n",
    "#         splited = line.split()\n",
    "#         #print(type(x))\n",
    "#         #print(x)\n",
    "    \n",
    "        \n",
    "        \n",
    "#         #print(file_line)\n",
    "    \n",
    "    \n",
    "# #     print(file_line)\n",
    "# #     print(file_line[0])\n",
    "# #     print(file_line[1])\n",
    "# #     print(file_line[2])\n",
    "# #     print(file_line[3])\n",
    "# #     file_line = file.readline()\n",
    "# #     print(file_line)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/relevance judgements.qrel\"\n",
    "# read_relevance_judgements(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_words_without_stop_words):\n",
    "    \"\"\"\n",
    "    This function takes in list of words which do not contain stop_words.\n",
    "    It uses the PorterStemmer() to reduce the word to their root words.\n",
    "    \n",
    "    Argument:\n",
    "    removed_all_stop_words -- list of all words which do not have stop_words.\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_words -- list of words which are reduced to their origin word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = list()\n",
    "    for w in tokenized_words_without_stop_words:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    stemmed_words.sort()\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query_string):\n",
    "    #splited_query = list(re.split(query))\n",
    "    splited_query = list(query_string.split())\n",
    "    #splited_query.lower()\n",
    "    cleaned_tokens_from_stop_words =  list(set(splited_query) - set(stop_words))\n",
    "    stemmed_tokens_of_words = stem_words(cleaned_tokens_from_stop_words)\n",
    "    return stemmed_tokens_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#print(type(qrys[str(202)]))\n",
    "x = query_processing(qrys[str(202)])\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Function 1: Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_documents_length(file_name):\n",
    "    doc_lengths = dict()\n",
    "    i = 0\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    \n",
    "    for each_line in file:\n",
    "        x = each_line.split()\n",
    "        doc_lengths[x[0]] = x[2]\n",
    "#        doc_frequency[each_line]\n",
    "        #print(x[0])\n",
    "    return doc_lengths \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "doc_length_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "all_doc_lengths = get_all_documents_length(doc_length_file)\n",
    "c = int(all_doc_lengths[str(3058)])\n",
    "#print(int(all_doc_lengths[str(3058)]))\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_vocablury(file_name):\n",
    "    vocablury = dict()\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    for each_line in file:\n",
    "        x = each_line.split()\n",
    "        #Interesting: making term as key and term_id as value (;\n",
    "        vocablury[x[1]] = x[0]\n",
    "    return vocablury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "vocablury = get_all_vocablury(voc_file)\n",
    "#print(vocablury)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_postings(file_name):\n",
    "    doc_freq = dict()\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    #print(\"to\")\n",
    "    for each_line in file:\n",
    "        lst = list()\n",
    "        x = each_line.split('\\t')\n",
    "        lineList = [line.rstrip('\\n') for line in x]\n",
    "        \n",
    "#         z = x[2].strip(\"\\n\")\n",
    "#         x = z.strip(\"[]\")\n",
    "#         lst = x.split(\",\")\n",
    "        \n",
    "        doc_freq[x[0]] = lineList\n",
    "        #print((lineList[0]))\n",
    "        #print((lineList[2]))\n",
    "    return doc_freq\n",
    "        \n",
    "# def iter_list(x, new_arr):\n",
    "#     if(isinstance(x,list)):\n",
    "#         iter_list(x, new_arr)\n",
    "#     else:\n",
    "#         return new_arr.append(x)\n",
    "            \n",
    "       \n",
    "    \n",
    "    \n",
    "#     with open(file_name) as f:\n",
    "#         return map(int, f)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# #        print(x)\n",
    "        #y = list(x[2])\n",
    "        #lst = list()\n",
    "        #for e in len(y):\n",
    "            #lst.append(y[e])\n",
    "        #doc_freq[x[0]] = lst\n",
    "    \n",
    "#     output = []\n",
    "#     with open(file_name, 'r') as f:\n",
    "#         w, h  = map(int, f.readline().split())\n",
    "#         tmp = []\n",
    "#         for i, line in enumerate(f):\n",
    "#             if i == h:\n",
    "#                 break\n",
    "#             tmp.append(map(int, line.split()[:w]))\n",
    "#         output.append(tmp)\n",
    "#     with open(file_name) as f:\n",
    "#         w , h = [int(x) for x in next(f).split()] # read first line\n",
    "#         array = []\n",
    "#         for line in f: # read rest of lines\n",
    "#             array.append([int(x) for x in line.split()])\n",
    "\n",
    "        \n",
    "        \n",
    "#         splitTextAlp = str(re.findall(r\"[0-9]+\", each_line))\n",
    "#         splitTextAlp = list(re.split(r'\\W+', (splitTextAlp)))\n",
    "#         print(splitTextAlp)\n",
    "\n",
    "        \n",
    "#         x = each_line.split('\\t')\n",
    "#         print(type(x))\n",
    "\n",
    "\n",
    "        #print(x[0])\n",
    "        #print(x[1])\n",
    "        \n",
    "    \n",
    "        #print(y)\n",
    "        #y = x[2].strip(\"\\n\")\n",
    "        #z = y.split(',')\n",
    "        #print(type(x[2]))\n",
    "        #lineList = [line.rstrip('\\n') for line in x]\n",
    "        #y = x[2].strip(\"\\n\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "#print(dict_termid_with_docs_postings)\n",
    "\n",
    "#print(dict_termid_with_docs_postings)\n",
    "for kys, values in dict_termid_with_docs_postings.items():\n",
    "    #print( dict_termid_with_docs_postings[kys])\n",
    "    c = dict_termid_with_docs_postings[kys]\n",
    "    z = c[2].strip(\"\\n\")\n",
    "    x = z.strip(\"[]\")\n",
    "    s = x.split(\",\")\n",
    "    #print((s[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequency(file_name):\n",
    "    term_freq = dict()\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    for each_line in file:\n",
    "        x = each_line.split(\"\\t\")\n",
    "        term_freq[x[0]] = x[2]\n",
    "    return term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_term_id = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "dict_term_id_with_frequencies = get_term_frequency(hashed_term_id)\n",
    "#print(dict_term_id_with_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[str(551445)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_okapi_bm25():\n",
    "    \n",
    "    k1 = 1.2 \n",
    "    k2 = 800\n",
    "    b = 0.75\n",
    "    D = 17\n",
    "    tf_q_i = 1\n",
    "    \n",
    "    queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "    queries_dict = xml_parser(queries_file_name) #returned -> dict\n",
    "    \n",
    "    doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "    all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "    voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "    vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "    doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "    dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "    cleared_dict_termid_with_docs_postings = dict()\n",
    "    \n",
    "    for kys, values in dict_termid_with_docs_postings.items():\n",
    "    #print( dict_termid_with_docs_postings[kys])\n",
    "        c = dict_termid_with_docs_postings[kys]\n",
    "        z = c[2].strip(\"\\n\")\n",
    "        x = z.strip(\"[]\")\n",
    "        s = x.split(\",\")\n",
    "        cleared_dict_termid_with_docs_postings[kys] = s\n",
    "    \n",
    "\n",
    "    hashed_term_id = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "    dict_term_id_with_frequencies = get_term_frequency(hashed_term_id)\n",
    "    \n",
    "    avg_length = 0\n",
    "    for doc_id, length in all_doc_lengths.items():\n",
    "        avg_length += int(length)\n",
    "        #print((avg_length))\n",
    "    avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "    scores_dictionary = dict()    \n",
    "    #capital_K = k1 * ((1-b) + (b * (len(d)/avgr_d)))\n",
    "    for query_id, query in queries_dict.items(): # run for 10 times\n",
    "        splitted_query = query_processing(query)\n",
    "        # Ab wo documents aa jaien jin mein ye term id hai.\n",
    "        # split kerne k bad, har term ka doc k sath score nikalna hai\n",
    "        score = 0\n",
    "        # query ki eik term, phr dosri, phr teesri and so on.\n",
    "        for i in range(0,len(splitted_query)):\n",
    "            #print(splitted_query[i])\n",
    "            # eik term pakar li\n",
    "            # ab dekhna hai wo term kitne docs mein mujood hai\n",
    "            #lkn pehle us term ka term_id le leya jae\n",
    "            docs_score_for_each_query = dict()\n",
    "            if splitted_query[i] in vocablury: # if that term exists\n",
    "                #then get its term_id\n",
    "                term_id = vocablury[splitted_query[i]]\n",
    "                # ab dekhna hai ye term ktne docs mein mujood hai\n",
    "                #print(term_id)\n",
    "                list_of_all_docs_in_which_term_exists = cleared_dict_termid_with_docs_postings[str(term_id)]\n",
    "                #print((list_of_all_docs_in_which_term_exists))\n",
    "                #print(len(list_of_all_docs_in_which_term_exists))\n",
    "                df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "                #tf_q_i = 1\n",
    "                tf_d_i = int(dict_term_id_with_frequencies[str(term_id)])\n",
    "                for i in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "                    doc_id = list_of_all_docs_in_which_term_exists[i]\n",
    "                    #print(doc_id)\n",
    "                    if doc_id in all_doc_lengths:\n",
    "                        length_of_doc_id = int(all_doc_lengths[str(doc_id)])\n",
    "                    \n",
    "                        capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "                        score += (float((math.log((D+0.5))/float(df_i+0.5))) * float(((float(1+k1)*tf_d_i)/float(capital_K+tf_d_i))) * float((((1+k2)*float(tf_q_i)/(k2+tf_q_i)))))\n",
    "                    docs_score_for_each_query[str(doc_id)] = score\n",
    "                #print(docs_score_for_each_query)\n",
    "                    \n",
    "            scores_dictionary[query_id] = dict()\n",
    "            scores_dictionary[query_id] = docs_score_for_each_query\n",
    "    return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'202': {},\n",
       " '214': {'7116': 4.626100392668382},\n",
       " '216': {'6448': 3.307155804139594,\n",
       "  ' 7116': 3.307155804139594,\n",
       "  ' 606': 3.307155804139594},\n",
       " '221': {'7170': 3.9278961368001486,\n",
       "  ' 6630': 3.9278961368001486,\n",
       "  ' 3660': 3.9278961368001486,\n",
       "  ' 6037': 3.9278961368001486,\n",
       "  ' 7038': 3.9278961368001486},\n",
       " '227': {'2782': 2.953631351311153, ' 6630': 2.953631351311153},\n",
       " '230': {},\n",
       " '234': {'2037': 8.212477667239865, ' 7455': 8.212477667239865},\n",
       " '243': {'5710': 3.769325960104702},\n",
       " '246': {},\n",
       " '250': {'3660': 3.7417829151391775}}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_okapi_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-239-48b12880a518>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-239-48b12880a518>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def dirichlet_smoothing():\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def dirichlet_smoothing():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensDict=dict()\n",
    "i=0\n",
    "file=open(\"termids.txt\",'r',encoding=\"utf-8\")\n",
    "for value in file:\n",
    "    value=value.split(\"/\")\n",
    "    value[1]=value[1].strip(\"\\n\")\n",
    "    tokensDict[str(value[1])]=str(value[0])\n",
    "\n",
    "    i=i+1\n",
    "file.close()\n",
    "\n",
    "###############################\n",
    "#Average Doc Length\n",
    "#i=0\n",
    "#TotalDocLength=0\n",
    "#file = open(\"doc_index.txt\",'r')\n",
    "#for value in file:\n",
    "#    i=i+1\n",
    "#    print(i)\n",
    "#    value=value.split(\"\\t\")\n",
    "#    TotalDocLength=TotalDocLength+(len(value)-2)\n",
    "\n",
    "#averageLength=TotalDocLength/3474\n",
    "#file.close()\n",
    "Mubeen\n",
    "file1 = open(\"term_index.txt\", 'rb')\n",
    "forwardIndex = {}\n",
    "queryDocList = []\n",
    "for i in range(0, len(titles)):\n",
    "    docsList = []\n",
    "    vector = []\n",
    "    for token in queries2[i].keys():\n",
    "        if token not in vector:\n",
    "            termid = tokensDict[token]\n",
    "            forwardIndex[termid]={}\n",
    "            file2 = open(\"term_info.txt\", 'r')\n",
    "            for rows in file2:\n",
    "                rows=rows.split(\"\\t\")\n",
    "                if str(rows[0])==str(termid):\n",
    "                    file1.seek(int(rows[1]))\n",
    "                    line=file1.readline().decode()\n",
    "                    line=line.split(\"\\t\")\n",
    "                    for k in range(1,len(line)):\n",
    "                        pair=line[k].split(\":\")\n",
    "                        if pair[0] not in forwardIndex[termid].keys():\n",
    "                            forwardIndex[termid][pair[0]]=1\n",
    "\n",
    "                        else:\n",
    "                            forwardIndex[termid][pair[0]]=(forwardIndex[termid][pair[0]])+1\n",
    "\n",
    "\n",
    "                        if pair[0] not in docsList:\n",
    "                            docsList.append(pair[0])\n",
    "\n",
    "                    break\n",
    "            file2.close()\n",
    "            vector.append(token)\n",
    "    queryDocList.append(docsList)\n",
    "file1.close()\n",
    "Mubeen\n",
    "file1 = open(\"term_index.txt\", 'rb')\n",
    "forwardIndex = {}\n",
    "queryDocList = []\n",
    "for i in range(0, len(titles)):\n",
    "    docsList = []\n",
    "    vector = []\n",
    "    for token in queries2[i].keys():\n",
    "        if token not in vector:\n",
    "            termid = tokensDict[token]\n",
    "            forwardIndex[termid]={}\n",
    "            file2 = open(\"term_info.txt\", 'r')\n",
    "            for rows in file2:\n",
    "                rows=rows.split(\"\\t\")\n",
    "                if str(rows[0])==str(termid):\n",
    "                    file1.seek(int(rows[1]))\n",
    "                    line=file1.readline().decode()\n",
    "                    line=line.split(\"\\t\")\n",
    "                    for k in range(1,len(line)):\n",
    "                        pair=line[k].split(\":\")\n",
    "                        if pair[0] not in forwardIndex[termid].keys():\n",
    "                            forwardIndex[termid][pair[0]]=1\n",
    "\n",
    "                        else:\n",
    "                            forwardIndex[termid][pair[0]]=(forwardIndex[termid][pair[0]])+1\n",
    "\n",
    "\n",
    "                        if pair[0] not in docsList:\n",
    "                            docsList.append(pair[0])\n",
    "\n",
    "                    break\n",
    "            file2.close()\n",
    "            vector.append(token)\n",
    "    queryDocList.append(docsList)\n",
    "file1.close()\n",
    "Mubeen\n",
    "for i in range(0, len(titles)):\n",
    "        topicID = str(titles[i].attrs['number'])\n",
    "        tuples=[]\n",
    "\n",
    "        for values in docsDict.keys():\n",
    "            if values in queryDocList[i]:\n",
    "                vector = []\n",
    "                docScores = []\n",
    "                queryScores = []\n",
    "                doclen=int(getTerms(values))\n",
    "                for token in queries2[i].keys():\n",
    "                    if token not in vector:\n",
    "                        vector.append(token)\n",
    "                        termid = tokensDict[token]\n",
    "                        # for query\n",
    "                        termFreqInQuery = 1\n",
    "                        score = termFreqInQuery / (termFreqInQuery + 0.5 + (1.5 * (doclen / averageLength)))\n",
    "                        queryScores.append(score)\n",
    "\n",
    "                        if values in forwardIndex[termid].keys():\n",
    "                            termFreqInDocument = forwardIndex[str(termid)][values]\n",
    "                            score = termFreqInDocument / (termFreqInDocument + 0.5 + (1.5 * (doclen / averageLength)))\n",
    "                            docScores.append(score)\n",
    "\n",
    "                        else:\n",
    "                            docScores.append(0)\n",
    "\n",
    "                numerator = 0\n",
    "                for j in range(0, len(docScores)):\n",
    "                    numerator = numerator + (docScores[j] * queryScores[j])\n",
    "\n",
    "                docVectorMagnitute = 0\n",
    "                for j in range(0, len(docScores)):\n",
    "                        docVectorMagnitute = docVectorMagnitute + (docScores[j] * docScores[j])\n",
    "\n",
    "                docVectorMagnitute = sqrt(docVectorMagnitute)\n",
    "\n",
    "                queryVectorMagnitute = 0\n",
    "                for j in range(0, len(queryScores)):\n",
    "                    queryVectorMagnitute = queryVectorMagnitute + (queryScores[j] * queryScores[j])\n",
    "\n",
    "                queryVectorMagnitute = sqrt(queryVectorMagnitute)\n",
    "\n",
    "                denominator = docVectorMagnitute * queryVectorMagnitute\n",
    "                if denominator==0:\n",
    "                    TF=0\n",
    "                else:\n",
    "                    TF = numerator / denominator\n",
    "                pair=[docsDict[values],TF]\n",
    "                tuples.append(pair)\n",
    "\n",
    "            else:\n",
    "                pair=[docsDict[values],0]\n",
    "                tuples.append(pair)\n",
    "\n",
    "        for j in range (0,len(tuples)-1):\n",
    "            min = tuples[j][1]\n",
    "            for k in range(j+1,len(tuples)):\n",
    "                if tuples[j][1] < tuples [k][1]:\n",
    "                    temp=tuples[j]\n",
    "                    tuples[j]=tuples[k]\n",
    "                    tuples[k]=temp\n",
    "\n",
    "        for j in range(0, len(tuples)):\n",
    "            print(str(topicID) + \" 0 \" + tuples[j][0] + \" \" + str(j + 1) + \" \" + str(tuples[j][1]) + \" run1\")\n",
    "Mubeen\n",
    "#making dictionary of docids DocumentID => [DocumentName]\n",
    "docsDict={}\n",
    "file=open(\"docids.txt\",'r')\n",
    "for value in file:\n",
    "    value=value.split(\"/\")\n",
    "    value[1]=value[1].strip(\"\\n\")\n",
    "    docsDict[str(value[0])]=value[1]\n",
    "file.close()\n",
    "Mubeen\n",
    "def getTerms(doc):\n",
    "\n",
    "    file=open(\"doc_index.txt\")\n",
    "\n",
    "    i=0\n",
    "    do=0\n",
    "    for value in file:\n",
    "        value=value.split(\"\\t\")\n",
    "\n",
    "        if do == 1 and doc != str(value[0]):\n",
    "            break\n",
    "\n",
    "        elif do == 1:\n",
    "            i=i+(len(value)-2)\n",
    "\n",
    "        elif doc == str(value[0]):\n",
    "            do=1\n",
    "            i = i + (len(value) - 2)\n",
    "\n",
    "    file.close()\n",
    "    return i\n",
    "\n",
    "def getTokenID(token):\n",
    "    file=open(\"termids.txt\",'r',encoding='utf-8')\n",
    "    for value in file:\n",
    "        value=value.split('/')\n",
    "        value[1]=value[1].strip('\\n')\n",
    "        if(value[1]==token):\n",
    "            file.close()\n",
    "            return value[0]\n",
    "\n",
    "    return -1\n",
    "\n",
    "def find(big,str2):\n",
    "    for str in big:\n",
    "        if (str==str2):\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "def termFreqInDoc(doc,term):\n",
    "\n",
    "    file = open(\"term_info.txt\", 'r')\n",
    "    offset = 0\n",
    "    for value in file:\n",
    "        value = value.split(\"\\t\")\n",
    "        if term == value[0]:\n",
    "            offset = value[1]\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    file = open(\"term_index.txt\", \"rb\")\n",
    "    file.seek(int(offset))\n",
    "    line = file.readline().decode()\n",
    "    line = line.split(\"\\t\")\n",
    "    freq = 0\n",
    "    for xx in line:\n",
    "        xx = xx.split(\":\")\n",
    "        if xx[0] == doc:\n",
    "            freq = freq + 1\n",
    "\n",
    "    file.close()\n",
    "    return freq\n",
    "\n",
    "def getDocID(token):\n",
    "        file = open(\"docids.txt\", 'r')\n",
    "        for value in file:\n",
    "            value = value.split('/')\n",
    "            value[1] = value[1].strip('\\n')\n",
    "            if (value[1] == token):\n",
    "                file.close()\n",
    "                return value[0]\n",
    "        return -1\n",
    "\n",
    "def docsContainingTerm(term):\n",
    "    file = open(\"term_info.txt\", 'r')\n",
    "\n",
    "    for value in file:\n",
    "        value = value.split(\"\\t\")\n",
    "        if (value[0] == term):\n",
    "            return value[3]\n",
    "\n",
    "    return 0\n",
    "    file.close()\n",
    "\n",
    "def termsInCorpus(term):\n",
    "    file=open(\"term_info.txt\",'r')\n",
    "\n",
    "    for value in file:\n",
    "        value=value.split(\"\\t\")\n",
    "        if(value[0]==term):\n",
    "            return value[2]\n",
    "\n",
    "    return 0\n",
    "    file.close()\n",
    "Mubeen\n",
    "for i in range(0, len(titles)):\n",
    "        topicID = str(titles[i].attrs['number'])\n",
    "        tuples=[]\n",
    "\n",
    "        for values in docsDict.keys():\n",
    "            if values in queryDocList[i]:\n",
    "                vector = []\n",
    "                docScores = []\n",
    "                queryScores = []\n",
    "                doclen=int(getTerms(values))\n",
    "                for token in queries2[i].keys():\n",
    "                    if token not in vector:\n",
    "\n",
    "                        IDF=log10(3474/int(docsContainingTerm(termid)))\n",
    "\n",
    "                        vector.append(token)\n",
    "                        termid = tokensDict[token]\n",
    "                        # for query\n",
    "                        termFreqInQuery = 1\n",
    "                        score = termFreqInQuery / (termFreqInQuery + 0.5 + (1.5 * (doclen / averageLength)))\n",
    "                        score=score*IDF\n",
    "                        queryScores.append(score)\n",
    "\n",
    "                        if values in forwardIndex[termid].keys():\n",
    "                            termFreqInDocument = forwardIndex[str(termid)][values]\n",
    "                            score = termFreqInDocument / (termFreqInDocument + 0.5 + (1.5 * (doclen / averageLength)))\n",
    "                            score = score * IDF\n",
    "                            docScores.append(score)\n",
    "\n",
    "                        else:\n",
    "                            docScores.append(0)\n",
    "\n",
    "                numerator = 0\n",
    "                for j in range(0, len(docScores)):\n",
    "                    numerator = numerator + (docScores[j] * queryScores[j])\n",
    "\n",
    "                docVectorMagnitute = 0\n",
    "                for j in range(0, len(docScores)):\n",
    "                        docVectorMagnitute = docVectorMagnitute + (docScores[j] * docScores[j])\n",
    "\n",
    "                docVectorMagnitute = sqrt(docVectorMagnitute)\n",
    "\n",
    "                queryVectorMagnitute = 0\n",
    "                for j in range(0, len(queryScores)):\n",
    "                    queryVectorMagnitute = queryVectorMagnitute + (queryScores[j] * queryScores[j])\n",
    "\n",
    "                queryVectorMagnitute = sqrt(queryVectorMagnitute)\n",
    "\n",
    "                denominator = docVectorMagnitute * queryVectorMagnitute\n",
    "                if denominator==0:\n",
    "                    TF=0\n",
    "                else:\n",
    "                    TF = numerator / denominator\n",
    "                pair=[docsDict[values],TF]\n",
    "                tuples.append(pair)\n",
    "\n",
    "            else:\n",
    "                pair=[docsDict[values],0]\n",
    "                tuples.append(pair)\n",
    "\n",
    "        for j in range (0,len(tuples)-1):\n",
    "            min = tuples[j][1]\n",
    "            for k in range(j+1,len(tuples)):\n",
    "                if tuples[j][1] < tuples [k][1]:\n",
    "                    temp=tuples[j]\n",
    "                    tuples[j]=tuples[k]\n",
    "                    tuples[k]=temp\n",
    "\n",
    "        for j in range(0, len(tuples)):\n",
    "            print(str(topicID) + \" 0 \" + tuples[j][0] + \" \" + str(j + 1) + \" \" + str(tuples[j][1]) + \" run1\")\n",
    "\n",
    "#########################################################################################################################################\n",
    "#########################################################################################################################################\n",
    "#########################################################################################################################################\n",
    "######################     Score = Okapi BM25  #######################################################################################\n",
    "#########################################################################################################################################\n",
    "elif sys.argv[2]==\"BM25\":\n",
    "\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k2 = 100\n",
    "\n",
    "    for i in range(0,len(titles)):\n",
    "        topicID=titles[i].attrs['number']\n",
    "        tuples = []\n",
    "\n",
    "        for values in docsDict.keys():\n",
    "            BM25 = 0\n",
    "            if values in queryDocList[i]:\n",
    "\n",
    "                doclen = getTerms(values)\n",
    "\n",
    "                K = k1 * ((1 - b) + (b * (int(doclen) / averageLength)))\n",
    "\n",
    "                for token in queries2[i].keys():\n",
    "\n",
    "                    termid = tokensDict[token]\n",
    "\n",
    "                    termFreqInQuery = 1\n",
    "                    if values in forwardIndex[termid].keys():\n",
    "                        termFreqInDocument = forwardIndex[termid][values]\n",
    "                        BM25 = BM25 + log10(        ((doclen + 0.5) / (int(docsContainingTerm(termid)) + 0.5))      *       (((1 + k1) * termFreqInDocument) / (K + termFreqInDocument))        *        ((((1 + k2) * termFreqInQuery) / (k2 + termFreqInQuery)))       )\n",
    "\n",
    "\n",
    "            pair = [docsDict[values], BM25]\n",
    "            tuples.append(pair)\n",
    "\n",
    "            #print(str(topicID) + \" 0 \" + docsDict[values] + \" \" + str(j + 1) + \" \" + str(BM25) + \" run1\")\n",
    "\n",
    "\n",
    "        for j in range(0, len(tuples) - 1):\n",
    "            min = tuples[j][1]\n",
    "            for k in range(j + 1, len(tuples)):\n",
    "                if tuples[j][1] < tuples[k][1]:\n",
    "                    temp = tuples[j]\n",
    "                    tuples[j] = tuples[k]\n",
    "                    tuples[k] = temp\n",
    "\n",
    "        for j in range(0, len(tuples)):\n",
    "            print(str(topicID) + \" 0 \" + tuples[j][0] + \" \" + str(j + 1) + \" \" + str(tuples[j][1]) + \" run1\")\n",
    "\n",
    "#########################################################################################################################################\n",
    "#########################################################################################################################################\n",
    "#########################################################################################################################################\n",
    "######################     Score = Jack Mercer Smoothing   ##############################################################################\n",
    "#########################################################################################################################################\n",
    "#########################################################################################################################################\n",
    "elif sys.argv[2]==\"JM\":\n",
    "\n",
    "    for i in range(0,len(titles)):\n",
    "        topicID=titles[i].attrs['number']\n",
    "        tuples = []\n",
    "\n",
    "        lemda=0.2\n",
    "\n",
    "        for values in docsDict.keys():\n",
    "            JM = 0\n",
    "            if values in queryDocList[i]:\n",
    "                doclen = getTerms(values)\n",
    "\n",
    "                for token in queries2[i].keys():\n",
    "\n",
    "                    termid = tokensDict[token]\n",
    "                    termFreqInQuery = 1\n",
    "\n",
    "                    if values in forwardIndex[termid].keys():\n",
    "\n",
    "                        if(JM==0):\n",
    "                            JM=     (lemda*( forwardIndex[termid][values]))     +       (( 1 - lemda )*(     int(termsInCorpus(termid))/totalDocLength    ))\n",
    "\n",
    "                        else:\n",
    "                            JM=JM*(     (lemda*( forwardIndex[termid][values]))     +   (( 1 - lemda )*(     int(termsInCorpus(termid))/totalDocLength    ))    )\n",
    "\n",
    "            pair=[docsDict[values],JM]\n",
    "            tuples.append(pair)\n",
    "\n",
    "\n",
    "\n",
    "        for j in range (0,len(tuples)-1):\n",
    "            min = tuples[j][1]\n",
    "            for k in range(j+1,len(tuples)):\n",
    "                if tuples[j][1] < tuples [k][1]:\n",
    "                    temp=tuples[j]\n",
    "                    tuples[j]=tuples[k]\n",
    "                    tuples[k]=temp\n",
    "\n",
    "\n",
    "        for j in range(0,len(tuples)):\n",
    "            print(str(topicID)+\" 0 \"+tuples[j][0]+\" \"+str(j+1)+\" \"+str(tuples[j][1])+\" run1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
