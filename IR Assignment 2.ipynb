{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: To get files for this assignment # 2, run inverted index 5.0 and get files from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will use the index you created in Assignment 1 to rank documents \n",
    "and create a search engine. You will implement two different scoring functions and compare \n",
    "their results against a baseline ranking produced by expert analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this assignment, you will need the following two files:\n",
    "\n",
    "<font color=red>  </font> <font color=blue> topics.xml (\\\\sandata\\xeon\\Maryam Bashir\\Information Retrieval\\topics.xml) </font> contains the queries you will be testing. \n",
    "\n",
    "You should run the queries using the text stored in the <font color=green> query </font> elements. The <font color=green> description </font> elements are only there to clarify the information need which the query is trying to express</font> .\n",
    "\n",
    "\n",
    "<font color=red>  </font> <font color=blue> corpus.qrel (\\\\sandata\\xeon\\Maryam Bashir\\Information Retrieval\\corpus.qrel)</font> contains the relevance grades from expert assessors. While these grades are not necessarily entirely correct (and defining correctness unambiguously is quite difficult), they are fairly reliable and we will treat them as being correct here. \n",
    "\n",
    "The format here is:\n",
    "<font color=green> topic </font> <font color=green> 0 </font> <font color=green> docid </font> <font color=green> grade </font>\n",
    "\n",
    "<font color=red> o </font> <font color=green> topic </font> is the ID of the query for which the document was assessed.\n",
    "\n",
    "<font color=red> o </font> <font color=green> 0 </font> is part of the format and can be ignored.\n",
    "\n",
    "<font color=red> o </font> <font color=green> docid </font> is the name of one of the documents which you have indexed.\n",
    "\n",
    "<font color=red> o </font> <font color=green> grade </font> is a value in the set <font color=blue> {-2, 0, 1, 2, 3, 4} </font>, where a higher value means that the document is more relevant to the query. \n",
    "The value -2 indicates a spam document, and 0 indicates a non-spam document which is completely non- relevant. \n",
    "Most queries do not have any document with a grade of 4, and many queries do not have any document with a grade of 3.\n",
    "This is a consequence of the specific meaning assigned to these grades here and the manner in which the documents were collected.\n",
    "\n",
    "This <font color=green> QREL </font> does not have assessments for every  <font color=blue>(query, document) </font> pair. If an assessment is missing, we assume the correct grade for the pair is 0 (non-relevant).\n",
    "\n",
    "You will write a program which takes the name of a scoring function as a command line argument and which prints a ranked list of documents for all queries found in topics.xml using that scoring function. \n",
    "\n",
    "For example:\n",
    "\n",
    "<font color=red> $ </font>  <font color=green> ./query.py --score TF-IDF </font> \n",
    "\n",
    "<font color=blue> 202 clueweb12-0000tw-13-04988 1 0.73 run1 </font> \n",
    "\n",
    "<font color=blue> 202 clueweb12-0000tw-13-04901 2 0.33 run1 </font>  \n",
    "\n",
    "<font color=blue> 202 clueweb12-0000tw-13-04932 3 0.32 run1 </font>  ...\n",
    "\n",
    "<font color=blue> 214 clueweb12-0000tw-13-05088 1 0.73 run1 </font> \n",
    "\n",
    "<font color=blue> 214 clueweb12-0000tw-13-05001 2 0.33 run1 </font> \n",
    "\n",
    "<font color=blue> 214 clueweb12-0000tw-13-05032 3 0.32 run1 </font> ...\n",
    "\n",
    "<font color=blue> 250 clueweb12-0000tw-13-05032 500 0.002 run1 </font>\n",
    "\n",
    "\n",
    "The output should have one row for each document which your program ranks for each query it runs. \n",
    "These lines should have the format:\n",
    "\n",
    "<font color=green> topic </font> <font color=green> docid </font> <font color=green> rank </font> <font color=green> score </font> <font color=green> run </font>\n",
    "\n",
    "<font color=red>  </font> <font color=green> topic </font> is the ID of the query for which the document was ranked.\n",
    "\n",
    "<font color=red>  </font> <font color=green> docid </font> is the document identifier.\n",
    "\n",
    "<font color=red>  </font> <font color=green> rank </font> is the order in which to present the document to the user. The document with the highest score will be assigned a rank of 1, the second highest a rank of 2, and so on.\n",
    "\n",
    "<font color=red>  </font> <font color=green> score </font> is the actual score the document obtained for that query.\n",
    "\n",
    "<font color=red>  </font> <font color=green> run </font> is the name of the run. You can use any value here. It is meant to allow research teams to submit multiple runs for evaluation in competitions such as TREC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "import xml.dom.minidom\n",
    "import numpy as np\n",
    "import math\n",
    "#from sets import Set\n",
    "#from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(mode):\n",
    "    \"\"\"\n",
    "    It takes only path of folder, no file name.\n",
    "    It only returns the folder which contain all the text file.\n",
    "    \n",
    "    Argument:\n",
    "    #nothing\n",
    "    \n",
    "    Returns:\n",
    "    dp -- directory path which contains all the txt files.\n",
    "    \"\"\"\n",
    "    if (mode == \"input\"):   \n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input\"\n",
    "    elif (mode == \"output\"):\n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/out\"\n",
    "    else:\n",
    "        raise ValueError('Unspecified mode for I/O.')\n",
    "        dp = None\n",
    "\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : read_stop_list\n",
    "def read_text_in_list_form(file_path):\n",
    "    \"\"\"\n",
    "    This function takes the path of stop words file and reads it and returns a list of words.\n",
    "    \n",
    "    Argument:\n",
    "    stop_file_path -- path should be like: path + file name.extension\n",
    "        \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\".\n",
    "    \n",
    "    Returns:\n",
    "    lineList -- list of words containg all the stop_words.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = [line.rstrip('\\n') for line in open(file_path)]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/stoplist.txt\"\n",
    "stop_words = read_text_in_list_form(stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function : To open term_ids with terms\n",
    "# def open_inverted_index(file_name):\n",
    "#     tokensDict = dict()\n",
    "#     i = 0\n",
    "#     file = open(file_name,'r',encoding = \"utf-8\")\n",
    "#     for value in file:\n",
    "#         value = value.split(\"/\")\n",
    "#         value[1] = value[1].strip(\"\\n\")\n",
    "#         tokensDict[str(value[1])] = str(value[0])\n",
    "#         i = i + 1\n",
    "#     file.close()\n",
    "#     return tokensDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xml_parser(file_name):\n",
    "#     doc = xml.dom.minidom.parse(file_name)\n",
    "#     qrys = doc.getElementsByTagName('query')\n",
    "#     tpcs = doc.getElementsByTagName('topic')\n",
    "#     queries_list = list()\n",
    "#     #qrys = mydoc.getElementsByTagName('topic')\n",
    "#     i = 0\n",
    "#     for elem in qrys:\n",
    "#         #print(tpcs[i].attributes['number'].value)\n",
    "#         queries_list.append(elem.firstChild.data)\n",
    "#         i = i + 1\n",
    "#         #print(elem.firstChild.data)\n",
    "#     #print(queries_list)\n",
    "\n",
    "## Version 2.0 using dictionary\n",
    "def xml_parser(file_name):\n",
    "    doc = xml.dom.minidom.parse(file_name)\n",
    "    qrys = doc.getElementsByTagName('query')\n",
    "    tpcs = doc.getElementsByTagName('topic')\n",
    "    queries = dict()\n",
    "    #qrys = mydoc.getElementsByTagName('topic')\n",
    "    i = 0\n",
    "    for elem in qrys:\n",
    "        queries[tpcs[i].attributes['number'].value] = elem.firstChild.data\n",
    "        #queries_list.append(elem.firstChild.data)\n",
    "        i = i + 1\n",
    "        #print(elem.firstChild.data)\n",
    "    #print(queries)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'202': 'uss carl vinson', '214': 'capital gains tax rate', '216': 'nicolas cage movies', '221': 'electoral college 2008 results', '227': 'i will survive lyrics', '230': \"world's biggest dog\", '234': 'dark chocolate health benefits', '243': 'afghanistan flag', '246': 'civil war battles in South Carolina', '250': 'ford edge problems'}\n"
     ]
    }
   ],
   "source": [
    "file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "qrys = xml_parser(file_name)\n",
    "#print(qrys[str(202)])\n",
    "print(qrys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_relevance_judgements(file_name):\n",
    "#     file = open(file_name,'r',encoding = \"utf-8\")\n",
    "#     for line in file:\n",
    "#         splited = line.split()\n",
    "#         #print(type(x))\n",
    "#         #print(x)\n",
    "    \n",
    "        \n",
    "        \n",
    "#         #print(file_line)\n",
    "    \n",
    "    \n",
    "# #     print(file_line)\n",
    "# #     print(file_line[0])\n",
    "# #     print(file_line[1])\n",
    "# #     print(file_line[2])\n",
    "# #     print(file_line[3])\n",
    "# #     file_line = file.readline()\n",
    "# #     print(file_line)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/relevance judgements.qrel\"\n",
    "# read_relevance_judgements(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any scoring function, you should process the text of the query in exactly the same way that you processed the text of a document. That is:\n",
    "1. Split the query into tokens (it is most correct to use the regular expression, but for these queries it suffices to split on whitespace)\n",
    "2. Convert all tokens to lowercase\n",
    "3. Apply stop-wording to the query using the same list you used in assignment 1\n",
    "4. Apply the same stemming algorithm to the query which you used in your indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(tokenized_words_without_stop_words):\n",
    "    \"\"\"\n",
    "    This function takes in list of words which do not contain stop_words.\n",
    "    It uses the PorterStemmer() to reduce the word to their root words.\n",
    "    \n",
    "    Argument:\n",
    "    removed_all_stop_words -- list of all words which do not have stop_words.\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_words -- list of words which are reduced to their origin word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = list()\n",
    "    for w in tokenized_words_without_stop_words:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    stemmed_words.sort()\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query_string):\n",
    "    #splited_query = list(re.split(query))\n",
    "    splited_query = list(query_string.split())\n",
    "    #splited_query.lower()\n",
    "    cleaned_tokens_from_stop_words =  list(set(splited_query) - set(stop_words))\n",
    "    stemmed_tokens_of_words = stem_words(cleaned_tokens_from_stop_words)\n",
    "    return stemmed_tokens_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carl', 'uss', 'vinson']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#print(type(qrys[str(202)]))\n",
    "x = query_processing(qrys[str(202)])\n",
    "print((x))\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Function 1: Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement BM25 scores. This should use the following scoring function for document d and query q:\n",
    "    \n",
    "Where k1,k2, and b are constants. For start, you can use the values suggested in the lecture on BM25 (k1 = 1.2, k2 varies from 0 to 1000, b = 0.75). Feel free to experiment with different values for these\n",
    "constants to learn their effect and try to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_documents_length(docid_hashed_file):\n",
    "    doc_lengths = dict()\n",
    "    i = 0\n",
    "    file = open(docid_hashed_file,'r',encoding = \"utf-8\")\n",
    "    \n",
    "    for each_line in file:\n",
    "        x = each_line.split()\n",
    "        doc_lengths[x[0]] = x[2]\n",
    "#        doc_frequency[each_line]\n",
    "        #print(x[0])\n",
    "    return doc_lengths \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1059': '320', '1533': '390', '6941': '240', '299': '593', '234': '467', '4776': '485', '2658': '855', '6057': '466', '2649': '304', '5256': '701', '3718': '1086', '1885': '202', '4103': '308', '93': '39', '1171': '349', '2410': '233', '7212': '120'}\n"
     ]
    }
   ],
   "source": [
    "docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "all_doc_lengths = get_all_documents_length(docid_hashed_file)\n",
    "#c = int(all_doc_lengths[str(3058)])\n",
    "#print(int(all_doc_lengths[str(3058)]))\n",
    "print((all_doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_documents_name(docid_hashed_file):\n",
    "    doc_names = dict()\n",
    "    i = 0\n",
    "    file = open(docid_hashed_file,'r',encoding = \"utf-8\")\n",
    "    \n",
    "    for each_line in file:\n",
    "        x = each_line.split()\n",
    "        doc_names[x[0]] = x[1]\n",
    "#        doc_frequency[each_line]\n",
    "        #print(x[0])\n",
    "    return doc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1059': 'clueweb12-1202wb-26-10513', '1533': 'clueweb12-1102wb-73-18046', '6941': 'clueweb12-1505wb-68-30103', '299': 'clueweb12-0303wb-53-27200', '234': 'clueweb12-1905wb-44-08158', '4776': 'clueweb12-1012wb-63-19337', '2658': 'clueweb12-1118wb-77-23080', '6057': 'clueweb12-0800tw-39-05237', '2649': 'clueweb12-0211wb-75-04122', '5256': 'clueweb12-0001wb-96-10862', '3718': 'clueweb12-0200wb-25-11228', '1885': 'clueweb12-1905wb-14-19033', '4103': 'clueweb12-1302wb-14-07756', '93': 'clueweb12-1101wb-78-26737', '1171': 'clueweb12-1705wb-99-01272', '2410': 'clueweb12-1504wb-72-14179', '7212': 'clueweb12-1002wb-18-28831'}\n"
     ]
    }
   ],
   "source": [
    "docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "print((all_doc_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_vocablury(file_name):\n",
    "    vocablury = dict()\n",
    "    file = open(file_name,'r',encoding = \"utf-8\")\n",
    "    for each_line in file:\n",
    "        x = each_line.split()\n",
    "        #Interesting: making term as key and term_id as value (;\n",
    "        vocablury[x[1]] = x[0]\n",
    "    return vocablury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "vocablury = get_all_vocablury(voc_file)\n",
    "#print(vocablury)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### version 1.0\n",
    "# def get_document_postings(document_postings_file):\n",
    "#     doc_freq = dict()\n",
    "#     file = open(document_postings_file,'r',encoding = \"utf-8\")\n",
    "#     #print(\"to\")\n",
    "#     for each_line in file:\n",
    "#         x = each_line.split('\\t')\n",
    "#         lineList = [line.rstrip('\\n') for line in x]\n",
    "#         doc_freq[x[0]] = lineList\n",
    "#     return doc_freq\n",
    "\n",
    "#### from below here It was next cell in version 1.0\n",
    "\n",
    "# doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "# dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "# #print(dict_termid_with_docs_postings)\n",
    "\n",
    "# #print(dict_termid_with_docs_postings)\n",
    "# for kys, values in dict_termid_with_docs_postings.items():\n",
    "#     #print( dict_termid_with_docs_postings[kys])\n",
    "#     c = dict_termid_with_docs_postings[kys]\n",
    "#     z = c[2].strip(\"\\n\")\n",
    "#     x = z.strip(\"[]\")\n",
    "#     s = x.split(\",\")\n",
    "#     print((s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### version 2.0\n",
    "def get_document_postings(document_postings_file):\n",
    "    termid_with_doc_postings = dict()\n",
    "    file = open(document_postings_file,'r',encoding = \"utf-8\")\n",
    "    for each_line in file:\n",
    "        x = (re.split(\"\\n\",each_line))\n",
    "        #print(\"x = \" , (x), \" len of x = \" ,len(x))\n",
    "        y = (re.split(\"\\t\", x[0]))\n",
    "        #print(\"y = \" , (y), \" len of y = \" ,len(y))\n",
    "        \n",
    "        temp_str = y[(len(y)-1)]\n",
    "        temp_str = temp_str.strip(\"[]\")\n",
    "        temp_str = temp_str.replace(\" \", \"\")\n",
    "        \n",
    "        lst = list()\n",
    "        lst = temp_str.split(\",\")\n",
    "        \n",
    "        current_term_id = y[0]\n",
    "        # create a key in dict_with_docid_and_its_positions\n",
    "        termid_with_doc_postings[current_term_id] = lst\n",
    "    return termid_with_doc_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "#print(dict_termid_with_docs_postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1.0\n",
    "# def get_term_frequency(term_index_hashed_file):\n",
    "#     dict_with_termid_and_its_occurance = dict()\n",
    "#     file = open(term_index_hashed_file, 'r' , encoding = \"utf-8\")\n",
    "#     length = 0\n",
    "#     current_term_id = 0\n",
    "#     for each_line in file:\n",
    "#         #print(\"each_line = \", (each_line))\n",
    "#         x = (re.split(\"\\n\",each_line))\n",
    "#         #print(\"x = \" , (x), \" len of x = \" ,len(x))\n",
    "#         y = (re.split(\"\\t\", x[0]))\n",
    "        \n",
    "#         if y[0] == '':\n",
    "#             y = y[1:]\n",
    "#         #print((\"y = \" ,y ))\n",
    "#         #print(\"Value at y[len]-1 = \" , (y[(len(y)-1)]), \" len(y[(len(y)-1)]) = \", len(y[(len(y)-1)]))\n",
    "        \n",
    "#         # It means a New term_id aya hai, tou new dict ki key bnani\n",
    "#         if (len(y) == 4):\n",
    "#             temp_str = y[(len(y)-1)]\n",
    "#             temp_str = temp_str.strip(\"[]\")\n",
    "#             lst = list()\n",
    "#             lst = temp_str.split(\",\")\n",
    "#             #print(\"temp_str in IF = \", type(temp_str))\n",
    "#             #print(\"lst in IF = \", lst, \" len(lst) = \", len(lst))\n",
    "            \n",
    "#             # reset length variable\n",
    "#             length = 0\n",
    "#             # new term_id now found\n",
    "#             current_term_id = y[0]\n",
    "#             # create a key in dict_with_termid_and_its_occurance\n",
    "#             dict_with_termid_and_its_occurance[current_term_id ] = int()\n",
    "            \n",
    "#             length = len(lst) #len((y[(len(y)-1)]))\n",
    "#             dict_with_termid_and_its_occurance[current_term_id] = length\n",
    "            \n",
    "#         elif (len(y) == 3):\n",
    "            \n",
    "#             temp_str = y[(len(y)-1)]\n",
    "#             temp_str = temp_str.strip(\"[]\")\n",
    "#             lst = list()\n",
    "#             lst = temp_str.split(\",\")\n",
    "#             #print(\"temp_str in ELSE = \", type(temp_str))\n",
    "#             #print(\"lst in ELSE = \", lst, \" len(lst) = \", len(lst))\n",
    "            \n",
    "#             length += len(lst) #len(y[(len(y)-1)])\n",
    "#             dict_with_termid_and_its_occurance[current_term_id] = length\n",
    "#         else:\n",
    "#             print(\"\\nI Don't know what to do.\\n\")\n",
    "#         #print('\\n')    \n",
    "#     return dict_with_termid_and_its_occurance   \n",
    "#         #y[3] = y[3].strip(\"[]\")\n",
    "        \n",
    "#         #y = str(y)\n",
    "#         #y = y.strip(\"''\")\n",
    "#         #y = y.strip(\"[]\")\n",
    "#         #print(\"y = \" , (y), \" len of y = \" ,len(y))\n",
    "#         #z = y.split(\",\")\n",
    "        \n",
    "#         #if z[0] == \"''\":\n",
    "#          #   z = z[1:]\n",
    "#         #- same term_id\n",
    "#         #- case 1: agar eik hi doc mein zyada pos hon\n",
    "#         #- case 2: agar eik doc mein 1 hi pos hon\n",
    "        \n",
    "#         #jab naya term_id aye to doc_contaienr wali new dict bnani ha\n",
    "#         #if (len(z)) == 3:\n",
    "        \n",
    "#         # z[0] =  per term_id hoga ya doc_id\n",
    "#         # z[1] = per hamesha]\n",
    "            \n",
    "#         #print(\"z = \" , (z), \" len of z = \" ,len(z))\n",
    "#         #term_freq[x[0]] = x[2]\n",
    "#     #return term_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2.0\n",
    "def get_term_frequency(term_index_hashed_file):\n",
    "    nested_dict_with_termid_and_its_docs_and_occurance = dict()\n",
    "    doc_id_with_positions = dict()\n",
    "    file = open(term_index_hashed_file, 'r' , encoding = \"utf-8\")\n",
    "    length = 0\n",
    "    current_term_id = 0\n",
    "    for each_line in file:\n",
    "        #print(\"each_line = \", (each_line))\n",
    "        x = (re.split(\"\\n\",each_line))\n",
    "        #print(\"x = \" , (x), \" len of x = \" ,len(x))\n",
    "        y = (re.split(\"\\t\", x[0]))\n",
    "        \n",
    "        if y[0] == '':\n",
    "            y = y[1:]\n",
    "        #print((\"y = \" ,y ))\n",
    "        #print(\"Value at y[len]-1 = \" , (y[(len(y)-1)]), \" len(y[(len(y)-1)]) = \", len(y[(len(y)-1)]))\n",
    "        \n",
    "        # It means a New term_id aya hai, tou dict ki new key bnani\n",
    "        if (len(y) == 4):\n",
    "            temp_str = y[(len(y)-1)]\n",
    "            temp_str = temp_str.strip(\"[]\")\n",
    "            temp_str = temp_str.replace(\" \", \"\")\n",
    "            lst = list()\n",
    "            lst = temp_str.split(\",\")\n",
    "            #print(\"temp_str in IF = \", type(temp_str))\n",
    "            #print(\"lst in IF = \", lst, \" len(lst) = \", len(lst))\n",
    "            doc_id_with_positions = dict()\n",
    "            # reset length variable\n",
    "            length = 0\n",
    "            # new term_id now found\n",
    "            current_term_id = y[0]\n",
    "            # create a key in dict_with_docid_and_its_positions\n",
    "            doc_id_with_positions[y[1]] = lst\n",
    "            \n",
    "            #length = len(lst) #len((y[(len(y)-1)]))\n",
    "            nested_dict_with_termid_and_its_docs_and_occurance[current_term_id] = doc_id_with_positions\n",
    "            \n",
    "        elif (len(y) == 3):\n",
    "            \n",
    "            temp_str = y[(len(y)-1)]\n",
    "            temp_str = temp_str.strip(\"[]\")\n",
    "            temp_str = temp_str.replace(\" \", \"\")\n",
    "            lst = list()\n",
    "            lst = temp_str.split(\",\")\n",
    "            #print(\"temp_str in ELSE = \", type(temp_str))\n",
    "            #print(\"lst in ELSE = \", lst, \" len(lst) = \", len(lst))\n",
    "            \n",
    "            doc_id_with_positions[y[0]] = lst\n",
    "            #length += len(lst) #len(y[(len(y)-1)])\n",
    "            nested_dict_with_termid_and_its_docs_and_occurance.update({current_term_id:doc_id_with_positions})\n",
    "        else:\n",
    "            print(\"\\nI Don't know what to do.\\n\")\n",
    "        #print('\\n')    \n",
    "    return nested_dict_with_termid_and_its_docs_and_occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1059': ['258', '259'], '1533': ['288'], '6941': ['199'], '299': ['446'], '234': ['360', '361'], '2658': ['706'], '5256': ['553', '554'], '3718': ['904', '905'], '4103': ['245']}\n"
     ]
    }
   ],
   "source": [
    "hashed_term_id = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "dict_term_id_with_frequencies = get_term_frequency(hashed_term_id)\n",
    "\n",
    "print((dict_term_id_with_frequencies[str(583007)]))\n",
    "#print((dict_term_id_with_frequencies[str(583007)][str(5256)]))\n",
    "#print(len(dict_term_id_with_frequencies[str(583007)][str(5256)]))\n",
    "#print(dict_term_id_with_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 1.0\n",
    "# ## Sort-> how ??\n",
    "# ## Scores > 0 ?? \n",
    "# ## Next ??\n",
    "# def calculate_okapi_bm25():\n",
    "#     k1 = 1.2 \n",
    "#     k2 = 300\n",
    "#     b = 0.75\n",
    "#     D = 17\n",
    "#     tf_q_i = 1\n",
    "    \n",
    "#     queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "#     queries_dict = xml_parser(queries_file_name) #returned -> dict\n",
    "    \n",
    "#     doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "#     vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "#     dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "#     cleared_dict_termid_with_docs_postings = dict()\n",
    "#     for kys, values in dict_termid_with_docs_postings.items():\n",
    "#     #print( dict_termid_with_docs_postings[kys])\n",
    "#         c = dict_termid_with_docs_postings[kys]\n",
    "#         z = c[2].strip(\"\\n\")\n",
    "#         x = z.strip(\"[]\")\n",
    "#         s = x.split(\",\")\n",
    "#         cleared_dict_termid_with_docs_postings[kys] = s\n",
    "    \n",
    "#     inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "#     dict_term_id_with_frequencies = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "#     avg_length = 0\n",
    "#     for doc_id, length in all_doc_lengths.items():\n",
    "#         avg_length += int(length)\n",
    "#         #print((avg_length))\n",
    "#     avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "#     scores_dictionary = dict()    \n",
    "#     #capital_K = k1 * ((1-b) + (b * (len(d)/avgr_d)))\n",
    "#     for query_id, query in queries_dict.items(): # run for 10 times\n",
    "#         splitted_query = query_processing(query)\n",
    "#         # Ab wo documents aa jaien jin mein ye term id hai.\n",
    "#         # split kerne k bad, har term ka doc k sath score nikalna hai\n",
    "#         score_of_each_term_for_single_doc = 0\n",
    "#         accumulated_score_for_query = 0\n",
    "#         # query ki eik term, phr dosri, phr teesri and so on.\n",
    "#         for i in range(0,len(splitted_query)):\n",
    "#             accumulated_score_for_one_term_in_multiple_docs = 0\n",
    "#             #print(splitted_query[i])\n",
    "#             # eik term pakar li\n",
    "#             # ab dekhna hai wo term kitne docs mein mujood hai\n",
    "#             #lkn pehle us term ka term_id le leya jae\n",
    "#             docs_score_for_each_query = dict()\n",
    "#             if splitted_query[i] in vocablury: # if that term exists\n",
    "#                 #then get its term_id\n",
    "#                 term_id = vocablury[splitted_query[i]]\n",
    "#                 # ab dekhna hai ye term ktne docs mein mujood hai\n",
    "#                 #print(term_id)\n",
    "#                 list_of_all_docs_in_which_term_exists = cleared_dict_termid_with_docs_postings[str(term_id)]\n",
    "#                 #print((list_of_all_docs_in_which_term_exists))\n",
    "#                 #print(len(list_of_all_docs_in_which_term_exists))\n",
    "#                 df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "#                 #tf_q_i = 1\n",
    "#                 tf_d_i = int(dict_term_id_with_frequencies[str(term_id)])\n",
    "#                 #doc_score_of_each_term_in_query_in_docs = dict()\n",
    "#                 # Eik term ki document wise.\n",
    "#                 for i in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "#                     # wo document utha leya jisme ye term mujood hai.\n",
    "#                     doc_id = list_of_all_docs_in_which_term_exists[i]\n",
    "#                     #print(doc_id)\n",
    "#                     if doc_id in all_doc_lengths: #security check if that doc_id is present in my doc_postings file\n",
    "#                         length_of_doc_id = int(all_doc_lengths[str(doc_id)])\n",
    "#                         capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "#                         score_of_each_term_for_single_doc = (float((math.log((D+0.5))/float(df_i+0.5))) * float(((float(1+k1)*tf_d_i)/float(capital_K+tf_d_i))) * float((((1+k2)*float(tf_q_i)/(k2+tf_q_i)))))\n",
    "#                         #accumulated_score_for_one_term_in_multiple_docs += score_of_each_term_for_single_doc\n",
    "#                         doc_name = all_doc_names[str(doc_id)]\n",
    "#                     #docs_score_for_each_query[str(doc_id)] = score_of_each_term_for_single_doc\n",
    "#                         #check if doc_id key is already present, kia pata ksi term k ne us doc per\n",
    "#                         # pehle apni value rakhwai ho\n",
    "#                         if str(doc_id) in docs_score_for_each_query: #pehle se mujood hai\n",
    "#                             prev_score = docs_score_for_each_query[str(doc_id)]\n",
    "#                             new_score = prev_score + score_of_each_term_for_single_doc\n",
    "#                             docs_score_for_each_query[doc_name] = new_score\n",
    "#                         else:\n",
    "#                             docs_score_for_each_query[str(doc_id)] = int()\n",
    "#                             docs_score_for_each_query[str(doc_id)] = score_of_each_term_for_single_doc\n",
    "#                     else:\n",
    "#                         docs_score_for_each_query[str(doc_id)] = 0\n",
    "#                 #accumulated_score_for_query += accumulated_score_for_one_term_in_multiple_docs\n",
    "#             else: \n",
    "#                 docs_score_for_each_query[str(doc_id)] = 0\n",
    "#                 #print(docs_score_for_each_query)\n",
    "                    \n",
    "#             scores_dictionary[query_id] = dict()\n",
    "#             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#     return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### version 2.0\n",
    "# ## Sort-> how ??\n",
    "# ## Scores > 0 ?? \n",
    "# ## Next ??\n",
    "# def calculate_okapi_bm25():\n",
    "#     k1 = 1.2 \n",
    "#     k2 = 300\n",
    "#     b = 0.75\n",
    "#     D = 17\n",
    "#     tf_q_i = 1\n",
    "    \n",
    "#     queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "#     queries_dict = xml_parser(queries_file_name) #returned -> dict\n",
    "    \n",
    "#     doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "#     vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "#     dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "#     cleared_dict_termid_with_docs_postings = dict()\n",
    "#     for kys, values in dict_termid_with_docs_postings.items():\n",
    "#     #print( dict_termid_with_docs_postings[kys])\n",
    "#         c = dict_termid_with_docs_postings[kys]\n",
    "#         z = c[2].strip(\"\\n\")\n",
    "#         x = z.strip(\"[]\")\n",
    "#         s = x.split(\",\")\n",
    "#         cleared_dict_termid_with_docs_postings[kys] = s\n",
    "    \n",
    "#     inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "#     dict_term_id_with_docs_and_positions = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "#     avg_length = 0\n",
    "#     for doc_id, length in all_doc_lengths.items():\n",
    "#         avg_length += int(length)\n",
    "#         #print((avg_length))\n",
    "#     avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "#     scores_dictionary = dict()    \n",
    "#     #capital_K = k1 * ((1-b) + (b * (len(d)/avgr_d)))\n",
    "#     for query_id, query in queries_dict.items(): # run for 10 times\n",
    "#         splitted_query = query_processing(query)\n",
    "#         # Ab wo documents aa jaien jin mein ye term id hai.\n",
    "#         # split kerne k bad, har term ka doc k sath score nikalna hai\n",
    "#         score_of_each_term_for_single_doc = 0\n",
    "#         accumulated_score_for_query = 0\n",
    "#         # query ki eik term, phr dosri, phr teesri and so on.\n",
    "#         for i in range(0,len(splitted_query)):\n",
    "#             accumulated_score_for_one_term_in_multiple_docs = 0\n",
    "#             #print(splitted_query[i])\n",
    "#             # eik term pakar li\n",
    "#             # ab dekhna hai wo term kitne docs mein mujood hai\n",
    "#             #lkn pehle us term ka term_id le leya jae\n",
    "#             docs_score_for_each_query = dict()\n",
    "#             if splitted_query[i] in vocablury: # if that term exists\n",
    "#                 #then get its term_id\n",
    "#                 term_id = vocablury[splitted_query[i]]\n",
    "#                 # ab dekhna hai ye term ktne docs mein mujood hai\n",
    "#                 #print(term_id)\n",
    "#                 list_of_all_docs_in_which_term_exists = cleared_dict_termid_with_docs_postings[str(term_id)]\n",
    "#                 #print((list_of_all_docs_in_which_term_exists))\n",
    "#                 #print(len(list_of_all_docs_in_which_term_exists))\n",
    "#                 df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "#                 #tf_q_i = 1\n",
    "                \n",
    "#                 #doc_score_of_each_term_in_query_in_docs = dict()\n",
    "#                 # Eik term ki document wise.\n",
    "#                 for i in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "#                     # wo document utha leya jisme ye term mujood hai.\n",
    "#                     doc_id = list_of_all_docs_in_which_term_exists[i]\n",
    "#                     #print(doc_id)\n",
    "#                     if doc_id in all_doc_lengths: #security check if that doc_id is present in my doc_postings file\n",
    "#                         tf_d_i = len(dict_term_id_with_docs_and_positions[str(term_id)][str(doc_id)])\n",
    "#                         #print(\"tf_d_i\", tf_d_i)\n",
    "#                         length_of_doc_id = int(all_doc_lengths[str(doc_id)])\n",
    "#                         capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "#                         score_of_each_term_for_single_doc = (float((math.log((D+0.5))/float(df_i+0.5))) * float(((float(1+k1)*tf_d_i)/float(capital_K+tf_d_i))) * float((((1+k2)*float(tf_q_i)/(k2+tf_q_i)))))\n",
    "#                         #accumulated_score_for_one_term_in_multiple_docs += score_of_each_term_for_single_doc\n",
    "#                         doc_name = all_doc_names[str(doc_id)]\n",
    "#                     #docs_score_for_each_query[str(doc_id)] = score_of_each_term_for_single_doc\n",
    "#                         #check if doc_id key is already present, kia pata ksi term k ne us doc per\n",
    "#                         # pehle apni value rakhwai ho\n",
    "#                         if str(doc_id) in docs_score_for_each_query: #pehle se mujood hai\n",
    "#                             prev_score = docs_score_for_each_query[str(doc_id)]\n",
    "#                             new_score = prev_score + score_of_each_term_for_single_doc\n",
    "#                             docs_score_for_each_query[doc_name] = new_score\n",
    "#                         else:\n",
    "#                             docs_score_for_each_query[str(doc_id)] = int()\n",
    "#                             docs_score_for_each_query[str(doc_id)] = score_of_each_term_for_single_doc\n",
    "#                     else:\n",
    "#                         docs_score_for_each_query[str(doc_id)] = 0\n",
    "#                 #accumulated_score_for_query += accumulated_score_for_one_term_in_multiple_docs\n",
    "#             else: \n",
    "#                 docs_score_for_each_query[str(doc_id)] = 0\n",
    "#                 #print(docs_score_for_each_query)\n",
    "                    \n",
    "#             scores_dictionary[query_id] = dict()\n",
    "#             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#     return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### build version 2.1 : removed unnecessary comments & print statements, and added proper comments\n",
    "# def calculate_okapi_bm25():\n",
    "#     k1 = 1.2 \n",
    "#     k2 = 300\n",
    "#     b = 0.75\n",
    "#     D = 17\n",
    "    \n",
    "#     queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "#     queries_dict = xml_parser(queries_file_name) \n",
    "    \n",
    "#     doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "#     vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "#     dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "#     cleared_dict_termid_with_docs_postings = dict()\n",
    "#     for kys, values in dict_termid_with_docs_postings.items():\n",
    "#         c = dict_termid_with_docs_postings[kys]\n",
    "#         z = c[2].strip(\"\\n\")\n",
    "#         x = z.strip(\"[]\")\n",
    "#         s = x.split(\",\")\n",
    "#         cleared_dict_termid_with_docs_postings[kys] = s\n",
    "    \n",
    "#     inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "#     dict_term_id_with_docs_and_positions = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "#     avg_length = 0\n",
    "#     for doc_id, length in all_doc_lengths.items():\n",
    "#         avg_length += int(length)\n",
    "#     avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "#     scores_dictionary = dict()\n",
    "    \n",
    "#     # Will run for number of times of queries in topics.xml\n",
    "#     for query_id, query in queries_dict.items(): # run for 10 times\n",
    "#         # Split one query in terms\n",
    "#         splitted_query = query_processing(query)\n",
    "#         score_of_each_term_for_single_doc = 0\n",
    "        \n",
    "#         # For loop for each term in single queries,\n",
    "#         # i.e. if there is 3 word query, it will run for 3 times.\n",
    "#         for i in range(0,len(splitted_query)):            \n",
    "#             # Reset : docs_score_for_each_query.\n",
    "#             docs_score_for_each_query = dict()\n",
    "            \n",
    "#             if(splitted_query[i]  == 'carl'):\n",
    "#                 print(\"yes 1\")\n",
    "            \n",
    "#             # Check If this splitted term (from query) exists in my vocablury.\n",
    "#             if splitted_query[i] in vocablury: # if that term exists\n",
    "#                 # If YES, then get term_id of this splitted term (from query).\n",
    "#                 # Now, get value = term_id by passing term as key.\n",
    "#                 term_id = vocablury[splitted_query[i]]\n",
    "                \n",
    "#                 if(splitted_query[i]  == 'carl'):\n",
    "#                     print(\"yes 2 term_id = \", term_id)\n",
    "                \n",
    "#                 # Get list of documents in which this term exists\n",
    "#                 list_of_all_docs_in_which_term_exists = cleared_dict_termid_with_docs_postings[(term_id)]\n",
    "                \n",
    "#                 if(splitted_query[i]  == 'carl'):\n",
    "#                     print(\"yes 3 list_of_all_docs_in_which_term_exists = \", list_of_all_docs_in_which_term_exists)\n",
    "                \n",
    "#                 # Get it's document_frequency, i.e. In how many docs it is present.\n",
    "#                 df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "                \n",
    "#                 if(splitted_query[i]  == 'carl'):\n",
    "#                     print(\"yes 4 df = \", df_i)\n",
    "                \n",
    "#                 #doc_score_of_each_term_in_query_in_docs = dict()\n",
    "                \n",
    "#                 # Now, run this loop for all docs in which it is present.\n",
    "#                 # i.e if it is present in 3 docs, it will run for 3 \n",
    "#                 for j in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "#                     # Now, pick one by one doc_id, and compute score.\n",
    "#                     doc_id = list_of_all_docs_in_which_term_exists[j]\n",
    "                    \n",
    "#                     # Check IF that doc_id is present in my doc_postings file\n",
    "#                     if doc_id in all_doc_lengths:\n",
    "#                         # Get this term's frquency in this document\n",
    "#                         tf_q_i = 1\n",
    "#                         tf_d_i = len(dict_term_id_with_docs_and_positions[str(term_id)][str(doc_id)])\n",
    "#                         length_of_doc_id = int(all_doc_lengths[(doc_id)])\n",
    "#                         capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "                        \n",
    "#                         if(splitted_query[i]  == 'carl'):\n",
    "#                             print(\"yes 5 K = \", capital_K)\n",
    "\n",
    "#                         a = float(D + 0.5)\n",
    "#                         b = float(df_i + 0.5)\n",
    "#                         c = float(math.log(a/b)) ###\n",
    "#                         d = float((1+k1) * tf_d_i) ###\n",
    "#                         e = float(capital_K + tf_d_i) ###\n",
    "#                         f = float((1+k2) * tf_q_i) ###\n",
    "#                         g = float(k2+tf_q_i) ###\n",
    "                        \n",
    "#                         score_of_each_term_for_single_doc = c * (d/e) * (f/g)\n",
    "#                         doc_name = all_doc_names[(doc_id)]\n",
    "                        \n",
    "#                         if(splitted_query[i]  == 'carl'):\n",
    "#                             print(\"yes 6 score_of_each_term_for_single_doc = \", score_of_each_term_for_single_doc)\n",
    "                        \n",
    "#                         # Check IF already a term might have calculated score for this document (for same query).\n",
    "#                         # Or we can say, multiple term words of single query might present in same document.\n",
    "#                         # If YES: Else NO\n",
    "#                         if str(doc_id) in docs_score_for_each_query:\n",
    "#                             #print(\"han kch para tha\")\n",
    "#                             prev_score = docs_score_for_each_query[(doc_id)]\n",
    "#                             new_score = prev_score + score_of_each_term_for_single_doc\n",
    "#                             docs_score_for_each_query[doc_id] = new_score\n",
    "                            \n",
    "#                             if(splitted_query[i]  == 'carl'):\n",
    "#                                 print(\"yes 7 IF = \", new_score)\n",
    "                                \n",
    "#                             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "\n",
    "#                         # Or maybe we found new term in new document\n",
    "#                         else:\n",
    "#                             #print(\"sab naya hai\")\n",
    "#                             if(splitted_query[i]  == 'carl'):\n",
    "#                                 print(\"yes 7 ELSE = \", score_of_each_term_for_single_doc)\n",
    "                            \n",
    "#                             docs_score_for_each_query[(doc_id)] = int()\n",
    "#                             docs_score_for_each_query[(doc_id)] = score_of_each_term_for_single_doc\n",
    "                            \n",
    "#                             scores_dictionary[query_id] = dict()\n",
    "#                             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#                     # Or maybe there is a document which is not in my possession.\n",
    "#                     else:\n",
    "#                         if(splitted_query[i]  == 'carl'):\n",
    "#                                 print(\"yes 8 = \", 0)\n",
    "#                         docs_score_for_each_query[(doc_id)] = 0\n",
    "#                         scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#             # If this term is not in my Vocablury\n",
    "#             else: \n",
    "#                 #if(splitted_query[i]  == 'carl'):\n",
    "                \n",
    "#                 print(\"Terms of Queries which are not in my collection = \", splitted_query[i])\n",
    "                \n",
    "#                 #docs_score_for_each_query[(doc_id)] = 0\n",
    "#                 #print(docs_score_for_each_query)\n",
    "\n",
    "#     return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### build version 2.2 : final removed unnecessary comments & print statements, and added proper comments\n",
    "# def calculate_okapi_bm25(parameters):\n",
    "#     k1 = parameters['k1']\n",
    "#     k2 = parameters['k2']\n",
    "#     b = parameters['b']\n",
    "#     D = parameters['D']\n",
    "    \n",
    "#     queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "#     queries_dict = xml_parser(queries_file_name) \n",
    "    \n",
    "#     doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "#     docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "#     all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "#     voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "#     vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "#     doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "#     dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "#     cleared_dict_termid_with_docs_postings = dict()\n",
    "#     for kys, values in dict_termid_with_docs_postings.items():\n",
    "#         c = dict_termid_with_docs_postings[kys]\n",
    "#         z = c[2].strip(\"\\n\")\n",
    "#         x = z.strip(\"[]\")\n",
    "#         s = x.split(\",\")\n",
    "#         cleared_dict_termid_with_docs_postings[kys] = s\n",
    "    \n",
    "#     inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "#     dict_term_id_with_docs_and_positions = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "#     avg_length = 0\n",
    "#     for doc_id, length in all_doc_lengths.items():\n",
    "#         avg_length += int(length)\n",
    "#     avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "#     scores_dictionary = dict()\n",
    "    \n",
    "#     # Will run for number of times of queries in topics.xml\n",
    "#     for query_id, query in queries_dict.items(): # run for 10 times\n",
    "#         # Split one query in terms\n",
    "#         splitted_query = query_processing(query)\n",
    "#         score_of_each_term_for_single_doc = 0\n",
    "#         # Reset : docs_score_for_each_query.\n",
    "#         docs_score_for_each_query = dict()\n",
    "            \n",
    "#         # For loop for each term in single queries,\n",
    "#         # i.e. if there is 3 word query, it will run for 3 times.\n",
    "#         for i in range(0,len(splitted_query)):            \n",
    "#             # Check If this splitted term (from query) exists in my vocablury.\n",
    "#             if splitted_query[i] in vocablury: # if that term exists\n",
    "#                 # If YES, then get term_id of this splitted term (from query).\n",
    "#                 # Now, get value = term_id by passing term as key.\n",
    "#                 term_id = vocablury[splitted_query[i]]\n",
    "                \n",
    "#                 # Get list of documents in which this term exists\n",
    "#                 list_of_all_docs_in_which_term_exists = cleared_dict_termid_with_docs_postings[(term_id)]\n",
    "                \n",
    "#                 # Get it's document_frequency, i.e. In how many docs it is present.\n",
    "#                 df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "            \n",
    "#                 #doc_score_of_each_term_in_query_in_docs = dict()\n",
    "                \n",
    "#                 # Now, run this loop for all docs in which it is present.\n",
    "#                 # i.e if it is present in 3 docs, it will run for 3 \n",
    "#                 for j in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "#                     # Now, pick one by one doc_id, and compute score.\n",
    "#                     doc_id = list_of_all_docs_in_which_term_exists[j]\n",
    "                    \n",
    "#                     # Check IF that doc_id is present in my doc_postings file\n",
    "#                     if doc_id in all_doc_lengths:\n",
    "#                         # Get this term's frquency in this document\n",
    "#                         tf_q_i = 1\n",
    "#                         tf_d_i = len(dict_term_id_with_docs_and_positions[str(term_id)][(doc_id)])\n",
    "#                         length_of_doc_id = int(all_doc_lengths[(doc_id)])\n",
    "#                         capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "\n",
    "#                         a = float(D + 0.5)\n",
    "#                         b = float(df_i + 0.5)\n",
    "#                         c = float(math.log(a/b)) ###\n",
    "#                         d = float((1+k1) * tf_d_i) ###\n",
    "#                         e = float(capital_K + tf_d_i) ###\n",
    "#                         f = float((1+k2) * tf_q_i) ###\n",
    "#                         g = float(k2+tf_q_i) ###\n",
    "                        \n",
    "#                         score_of_each_term_for_single_doc = c * (d/e) * (f/g)\n",
    "#                         doc_name = all_doc_names[(doc_id)]\n",
    "                        \n",
    "#                         # Check IF already a term might have calculated score for this document (for same query).\n",
    "#                         # Or we can say, multiple term words of single query might present in same document.\n",
    "#                         # If YES: Else NO\n",
    "#                         if doc_id in docs_score_for_each_query:\n",
    "#                             prev_score = docs_score_for_each_query[(doc_id)]\n",
    "#                             new_score = prev_score + score_of_each_term_for_single_doc\n",
    "#                             docs_score_for_each_query[doc_id] = new_score\n",
    "                            \n",
    "#                             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "\n",
    "#                         # Or maybe we found new term in new document\n",
    "#                         else:\n",
    "#                             docs_score_for_each_query[(doc_id)] = int()\n",
    "#                             docs_score_for_each_query[(doc_id)] = score_of_each_term_for_single_doc\n",
    "                            \n",
    "#                             scores_dictionary[query_id] = dict()\n",
    "#                             scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#                     # Or maybe there is a document which is not in my possession.\n",
    "#                     else:\n",
    "#                         docs_score_for_each_query[(doc_id)] = 0\n",
    "#                         scores_dictionary[query_id] = docs_score_for_each_query\n",
    "#             # If this term is not in my Vocablury\n",
    "#             else: \n",
    "#                 print(\"Terms of Queries which are not in my collection = \", splitted_query[i])\n",
    "\n",
    "#     return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build version 2.3 :\n",
    "#Changes Made: \n",
    "# final removed unnecessary comments & print statements, and added proper comments & replaced\n",
    "# doc_id with doc_names as key of docs_score_for_each_query()\n",
    "def calculate_okapi_bm25(parameters):\n",
    "    k1 = parameters['k1']\n",
    "    k2 = parameters['k2']\n",
    "    b = parameters['b']\n",
    "    D = parameters['D']\n",
    "    \n",
    "    queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "    queries_dict = xml_parser(queries_file_name) \n",
    "    \n",
    "    doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "    all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "    docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "    all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "    voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "    vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "    doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "    dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "    inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "    dict_term_id_with_docs_and_positions = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "    avg_length = 0\n",
    "    for doc_id, length in all_doc_lengths.items():\n",
    "        avg_length += int(length)\n",
    "    avg_length /= len(all_doc_lengths) \n",
    "       \n",
    "    scores_dictionary = dict()\n",
    "    # Will run for number of times of queries in topics.xml\n",
    "    for query_id, query in queries_dict.items(): # run for 10 times\n",
    "        \n",
    "        # Split one query in terms\n",
    "        splitted_query = query_processing(query)\n",
    "        score_of_each_term_for_single_doc = 0\n",
    "        # Reset : docs_score_for_each_query.\n",
    "        docs_score_for_each_query = dict()\n",
    "        #scores_dictionary[query_id] = dict()    \n",
    "        # For loop for each term in single queries,\n",
    "        # i.e. if there is 3 word query, it will run for 3 times.\n",
    "        for i in range(0,len(splitted_query)):            \n",
    "            # Check If this splitted term (from query) exists in my vocablury.\n",
    "            if splitted_query[i] in vocablury: # if that term exists\n",
    "                # If YES, then get term_id of this splitted term (from query).\n",
    "                # Now, get value = term_id by passing term as key.\n",
    "                term_id = vocablury[splitted_query[i]]\n",
    "                \n",
    "                # Get list of documents in which this term exists\n",
    "                list_of_all_docs_in_which_term_exists = dict_termid_with_docs_postings[(term_id)]\n",
    "                \n",
    "                # Get it's document_frequency, i.e. In how many docs it is present.\n",
    "                df_i = len(list_of_all_docs_in_which_term_exists)\n",
    "            \n",
    "                #doc_score_of_each_term_in_query_in_docs = dict()\n",
    "                \n",
    "                # Now, run this loop for all docs in which it is present.\n",
    "                # i.e if it is present in 3 docs, it will run for 3 \n",
    "                for j in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "                    # Now, pick one by one doc_id, and compute score.\n",
    "                    doc_id = list_of_all_docs_in_which_term_exists[j]\n",
    "                    doc_name = all_doc_names[doc_id]\n",
    "                    # Check IF that doc_id is present in my doc_postings file\n",
    "                    if doc_id in all_doc_lengths:\n",
    "                        # Get this term's frquency in this document\n",
    "                        tf_d_i = len(dict_term_id_with_docs_and_positions[str(term_id)][(doc_id)])\n",
    "                        tf_q_i = 1\n",
    "                        length_of_doc_id = int(all_doc_lengths[(doc_id)])\n",
    "                        capital_K = k1 * ((1-b) + (b * (length_of_doc_id/avg_length)))\n",
    "                        a = float(D + 0.5)\n",
    "                        b = float(df_i + 0.5)\n",
    "                        c = float(math.log(a/b)) ###\n",
    "                        d = float((1+k1) * tf_d_i) ###\n",
    "                        e = float(capital_K + tf_d_i) ###\n",
    "                        f = float((1+k2) * tf_q_i) ###\n",
    "                        g = float(k2+tf_q_i) ###\n",
    "                        score_of_each_term_for_single_doc = c * (d/e) * (f/g)\n",
    "                        \n",
    "                        # Check IF already a term might have calculated score for this document (for same query).\n",
    "                        # Or we can say, multiple term words of single query might present in same document.\n",
    "                        # If YES: Else NO\n",
    "                        if doc_name in docs_score_for_each_query:\n",
    "                            prev_score = docs_score_for_each_query[doc_name]\n",
    "                            new_score = prev_score + score_of_each_term_for_single_doc\n",
    "                            docs_score_for_each_query.update({doc_name : new_score})\n",
    "                            sorted_docs_score_for_each_query = sorted(docs_score_for_each_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                        # Or maybe we found new document. Let's create a new key of doc_id on same query_id\n",
    "                        else:\n",
    "                            docs_score_for_each_query[doc_name] = int()\n",
    "                            docs_score_for_each_query[doc_name] = score_of_each_term_for_single_doc\n",
    "                            sorted_docs_score_for_each_query = sorted(docs_score_for_each_query.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                            #scores_dictionary[query_id] = dict()\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                    # Or maybe there is a document which is not in my possession.\n",
    "                    else:\n",
    "                        docs_score_for_each_query[doc_name] = 0\n",
    "                        scores_dictionary[query_id] = docs_score_for_each_query\n",
    "            # If this term is not in my Vocablury\n",
    "            else: \n",
    "                print(\"Terms of Queries which are not in my collection = \", splitted_query[i])\n",
    "\n",
    "    return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms of Queries which are not in my collection =  uss\n",
      "Terms of Queries which are not in my collection =  vinson\n",
      "Terms of Queries which are not in my collection =  gain\n",
      "Terms of Queries which are not in my collection =  2008\n",
      "Terms of Queries which are not in my collection =  dog\n",
      "Terms of Queries which are not in my collection =  world'\n",
      "Terms of Queries which are not in my collection =  war\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'202': [('clueweb12-1012wb-63-19337', 2.31304098769458)],\n",
       " '214': [('clueweb12-1905wb-14-19033', 3.3881091795214116),\n",
       "  ('clueweb12-0211wb-75-04122', 3.1340425731532746),\n",
       "  ('clueweb12-0800tw-39-05237', 2.1445981205836477),\n",
       "  ('clueweb12-0200wb-25-11228', 1.2341386021338656)],\n",
       " '216': [('clueweb12-1302wb-14-07756', 5.95190162460523),\n",
       "  ('clueweb12-1705wb-99-01272', 4.7808621413327375),\n",
       "  ('clueweb12-0800tw-39-05237', 4.1547868035711435),\n",
       "  ('clueweb12-0001wb-96-10862', 2.2510214596424722),\n",
       "  ('clueweb12-1905wb-44-08158', 0.8720184255815691),\n",
       "  ('clueweb12-0200wb-25-11228', 0.2882922342511921)],\n",
       " '221': [('clueweb12-1905wb-14-19033', 472.55739438597215),\n",
       "  ('clueweb12-0211wb-75-04122', 13.833527580491179),\n",
       "  ('clueweb12-0303wb-53-27200', 3.756757749538382),\n",
       "  ('clueweb12-1905wb-44-08158', 0.8720184255815691),\n",
       "  ('clueweb12-0200wb-25-11228', 0.2882922342511921)],\n",
       " '227': [('clueweb12-1012wb-63-19337', 3.1937314883666224),\n",
       "  ('clueweb12-1905wb-44-08158', 3.0258731499680662),\n",
       "  ('clueweb12-1202wb-26-10513', 2.9705560794476376)],\n",
       " '230': [('clueweb12-1302wb-14-07756', 3.3021659982602727),\n",
       "  ('clueweb12-0303wb-53-27200', 1.0337805596961749),\n",
       "  ('clueweb12-0200wb-25-11228', 0.4008703043421091)],\n",
       " '234': [('clueweb12-1505wb-68-30103', 26.918331684604418),\n",
       "  ('clueweb12-1302wb-14-07756', 3.0699902483833705),\n",
       "  ('clueweb12-1012wb-63-19337', 1.7308556536450934)],\n",
       " '243': [('clueweb12-1102wb-73-18046', 5.7924999866084885),\n",
       "  ('clueweb12-0200wb-25-11228', 0.6170693010669328)],\n",
       " '246': [('clueweb12-0211wb-75-04122', 12.62684556406853),\n",
       "  ('clueweb12-0800tw-39-05237', 1.6986748410864683),\n",
       "  ('clueweb12-0200wb-25-11228', 0.6891625385933011),\n",
       "  ('clueweb12-1118wb-77-23080', 0.6691056287678858),\n",
       "  ('clueweb12-0001wb-96-10862', 0.3865195736622574),\n",
       "  ('clueweb12-1504wb-72-14179', -3.405280337628643),\n",
       "  ('clueweb12-1101wb-78-26737', -11.069125113807848)],\n",
       " '250': [('clueweb12-1118wb-77-23080', 1.351345009155612),\n",
       "  ('clueweb12-1905wb-44-08158', 1.331987117262492),\n",
       "  ('clueweb12-0800tw-39-05237', 1.2190853520850042),\n",
       "  ('clueweb12-0001wb-96-10862', 0.8575086514566356),\n",
       "  ('clueweb12-0200wb-25-11228', 0.7790576697371862)]}"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = dict()\n",
    "parameters['k1'] = 1.2 \n",
    "parameters['k2'] = 300\n",
    "parameters['b'] = 0.75\n",
    "parameters['D'] = 17\n",
    "\n",
    "\n",
    "calculate_okapi_bm25(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "## version v3.0\n",
    "def dirichlet_smoothing():\n",
    "    \n",
    "    queries_file_name = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/topics.xml\"\n",
    "    queries_dict = xml_parser(queries_file_name) \n",
    "    \n",
    "    doc_info_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "    all_doc_lengths = get_all_documents_length(doc_info_file)\n",
    "    \n",
    "    docid_hashed_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/docid_hashed.txt\"\n",
    "    all_doc_names = get_all_documents_name(docid_hashed_file)\n",
    "    \n",
    "    voc_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/termid_hashed.txt\"\n",
    "    vocablury = get_all_vocablury(voc_file)\n",
    "    \n",
    "    doc_postings_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/document_postings.txt\"\n",
    "    dict_termid_with_docs_postings = get_document_postings(doc_postings_file)\n",
    "    \n",
    "    inverted_index_file = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw2/input/term_index_hashed.txt\"\n",
    "    dict_term_id_with_docs_and_positions = get_term_frequency(inverted_index_file)\n",
    "    \n",
    "    mu = 0\n",
    "    total_length_of_collection = 0\n",
    "    for doc_id, length in all_doc_lengths.items():\n",
    "        total_length_of_collection += int(length)\n",
    "    mu = total_length_of_collection/len(all_doc_lengths) \n",
    "    \n",
    "    scores_dictionary = dict()\n",
    "    # Will run for number of times of queries in topics.xml\n",
    "    for query_id, query in queries_dict.items(): # run for 10 times\n",
    "        # Split one query in terms\n",
    "        splitted_query = query_processing(query)\n",
    "        score_of_each_term_for_single_doc = 0\n",
    "        # Reset : docs_score_for_each_query.\n",
    "        docs_score_for_each_query = dict()\n",
    "        \n",
    "        # For loop for each term in single queries,\n",
    "        # i.e. if there is 3 word query, it will run for 3 times.\n",
    "        for i in range(0,len(splitted_query)):            \n",
    "            # Check If this splitted term (from query) exists in my vocablury.\n",
    "            if splitted_query[i] in vocablury: # if that term exists\n",
    "                # If YES, then get term_id of this splitted term (from query).\n",
    "                # Now, get value = term_id by passing term as key.\n",
    "                term_id = vocablury[splitted_query[i]]\n",
    "                \n",
    "                # Get list of documents in which this term exists.\n",
    "                list_of_all_docs_in_which_term_exists = dict_termid_with_docs_postings[(term_id)]\n",
    "                \n",
    "                # Count the number of times each word occurs in Corpora, divide by total length.\n",
    "                # Basically add-up all lengths of documents.\n",
    "                sum_of_term_in_whole_corpora = 0\n",
    "                for d_idx, positinos in dict_term_id_with_docs_and_positions[term_id].items():\n",
    "                    sum_of_term_in_whole_corpora += len(positinos)\n",
    "                prob_of_term_occuring_in_whole_corpora = sum_of_term_in_whole_corpora/total_length_of_collection\n",
    "                \n",
    "                # Now, run this loop for all docs in which it is present.\n",
    "                # i.e if it is present in 3 docs, it will run for 3.\n",
    "                for j in range(0,len(list_of_all_docs_in_which_term_exists)):\n",
    "                    # Now, pick one by one doc_id, and compute score.\n",
    "                    doc_id = list_of_all_docs_in_which_term_exists[j]\n",
    "                    doc_name = all_doc_names[doc_id]\n",
    "                    # Check IF that doc_id is present in my doc_postings file.\n",
    "                    if doc_id in all_doc_lengths:\n",
    "                        N = int(all_doc_lengths[(doc_id)]) # doc length\n",
    "                        lamdba = N/(N+mu)\n",
    "                        one_minus_lamdba = 1 - lamdba\n",
    "                        \n",
    "                        # Count the number of times word occurs in document, divide by document length.\n",
    "                        count_of_term_in_single_doc = len(dict_term_id_with_docs_and_positions[term_id][doc_id])\n",
    "                        prob_occuring_in_single_doc = count_of_term_in_single_doc / N\n",
    "                        \n",
    "                        score_of_each_term_for_single_doc = (lamdba * prob_occuring_in_single_doc) + (one_minus_lamdba * prob_of_term_occuring_in_whole_corpora)\n",
    "                                                \n",
    "                        # Check IF already a term might have calculated score for this document (for same query).\n",
    "                        # Or we can say, multiple term words of single query might present in same document.\n",
    "                        # If YES: Else NO\n",
    "                        if doc_name in docs_score_for_each_query:\n",
    "                            prev_score = docs_score_for_each_query[doc_name]\n",
    "                            new_score = prev_score + score_of_each_term_for_single_doc\n",
    "                            docs_score_for_each_query.update({doc_name : new_score})\n",
    "                            sorted_docs_score_for_each_query = sorted(docs_score_for_each_query.items(), key=operator.itemgetter(1), reverse=True)\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                        # Or maybe we found new document. Let's create a new key of doc_id on same query_id.\n",
    "                        else:\n",
    "                            docs_score_for_each_query[doc_name] = int()\n",
    "                            docs_score_for_each_query[doc_name] = score_of_each_term_for_single_doc\n",
    "                            sorted_docs_score_for_each_query = sorted(docs_score_for_each_query.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                            #scores_dictionary[query_id] = dict()\n",
    "                            scores_dictionary[query_id] = sorted_docs_score_for_each_query\n",
    "                    # Or maybe there is a document which is not in my possession.\n",
    "                    else:\n",
    "                        docs_score_for_each_query[doc_name] = 0\n",
    "                        scores_dictionary[query_id] = docs_score_for_each_query\n",
    "            # If this term is not in my Vocablury.\n",
    "            else: \n",
    "                print(\"Terms of Queries which are not in my collection = \", splitted_query[i])\n",
    "\n",
    "    return scores_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms of Queries which are not in my collection =  uss\n",
      "Terms of Queries which are not in my collection =  vinson\n",
      "Terms of Queries which are not in my collection =  gain\n",
      "Terms of Queries which are not in my collection =  2008\n",
      "Terms of Queries which are not in my collection =  dog\n",
      "Terms of Queries which are not in my collection =  world'\n",
      "Terms of Queries which are not in my collection =  war\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'202': [('clueweb12-1012wb-63-19337', 0.00116860351879504)],\n",
       " '214': [('clueweb12-1905wb-14-19033', 0.00179380664652568),\n",
       "  ('clueweb12-0211wb-75-04122', 0.001541457082589648),\n",
       "  ('clueweb12-0200wb-25-11228', 0.001483216237314598),\n",
       "  ('clueweb12-0800tw-39-05237', 0.0011936339522546418)],\n",
       " '216': [('clueweb12-0800tw-39-05237', 0.005371352785145889),\n",
       "  ('clueweb12-0001wb-96-10862', 0.004246395806028834),\n",
       "  ('clueweb12-1705wb-99-01272', 0.003131922694981285),\n",
       "  ('clueweb12-1302wb-14-07756', 0.0018557366467645638),\n",
       "  ('clueweb12-1905wb-44-08158', 0.0015234814863880242),\n",
       "  ('clueweb12-0200wb-25-11228', 0.000897736143637783)],\n",
       " '221': [('clueweb12-0303wb-53-27200', 0.006728928592145716),\n",
       "  ('clueweb12-1905wb-14-19033', 0.006136706948640484),\n",
       "  ('clueweb12-0211wb-75-04122', 0.005273405808859322),\n",
       "  ('clueweb12-1905wb-44-08158', 0.0014572431608928928),\n",
       "  ('clueweb12-0200wb-25-11228', 0.000858704137392662)],\n",
       " '227': [('clueweb12-1012wb-63-19337', 0.0037005778095176266),\n",
       "  ('clueweb12-1905wb-44-08158', 0.00264953301980526),\n",
       "  ('clueweb12-1202wb-26-10513', 0.001666931258929989)],\n",
       " '230': [('clueweb12-1302wb-14-07756', 0.001613684040664838),\n",
       "  ('clueweb12-0303wb-53-27200', 0.001160160102094089),\n",
       "  ('clueweb12-0200wb-25-11228', 0.00078064012490242)],\n",
       " '234': [('clueweb12-1505wb-68-30103', 0.006584801566114967),\n",
       "  ('clueweb12-1302wb-14-07756', 0.0015329998386315962),\n",
       "  ('clueweb12-1012wb-63-19337', 0.0012335259365058755)],\n",
       " '243': [('clueweb12-1102wb-73-18046', 0.003988975921090804),\n",
       "  ('clueweb12-0200wb-25-11228', 0.000741608118657299)],\n",
       " '246': [('clueweb12-1101wb-78-26737', 0.007799514128628052),\n",
       "  ('clueweb12-0211wb-75-04122', 0.004867759208177835),\n",
       "  ('clueweb12-1504wb-72-14179', 0.001978595197409839),\n",
       "  ('clueweb12-0200wb-25-11228', 0.001639344262295082),\n",
       "  ('clueweb12-0800tw-39-05237', 0.001259946949602122),\n",
       "  ('clueweb12-0001wb-96-10862', 0.0011533420707732635),\n",
       "  ('clueweb12-1118wb-77-23080', 0.0009219563914626839)],\n",
       " '250': [('clueweb12-1118wb-77-23080', 0.0017978149633522335),\n",
       "  ('clueweb12-0800tw-39-05237', 0.001326259946949602),\n",
       "  ('clueweb12-1905wb-44-08158', 0.00132476650990263),\n",
       "  ('clueweb12-0001wb-96-10862', 0.0009960681520314548),\n",
       "  ('clueweb12-0200wb-25-11228', 0.000702576112412178)]}"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirichlet_smoothing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To evaluate your results, we will write a program that computes mean average precision of the \n",
    "rank list of documents for different queries. The input to program will be the <font color=blue> qrel file \n",
    "(relevance judgments) </font> and scoring file that has rank list of documents. \n",
    "\n",
    "The output should be following measures: \n",
    "    \n",
    "<font color=red>  </font> <font color=green> P@5  </font>\n",
    "\n",
    "<font color=red>  </font> <font color=green> P@10 </font>\n",
    "\n",
    "<font color=red>  </font> <font color=green> P@20 </font>\n",
    "\n",
    "<font color=red>  </font> <font color=green> P@30 </font>\n",
    "\n",
    "<font color=red>  </font> <font color=green> MAP </font>\n",
    "\n",
    "These measures should be computed for each query. Average for all queries should also be computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
