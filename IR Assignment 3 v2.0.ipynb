{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=green> Question 1 (WordToVec) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need genism library for this question. You can install gensim using following steps.\n",
    "1. Go to pip folder in Python installation path (Find python installation path by running <font color=blue> “where python” </font> in command prompt (cmd) in windows)\n",
    "\n",
    "For example: <font color=red> cd C:\\Program Files\\Python\\Python36-32\\Scripts </font>\n",
    "2. Run <font color=green>“pip install gensim”</font>\n",
    "\n",
    "You can get help on using gensim for this homework from following link <font color=blue> https://radimrehurek.com/gensim/ </font>\n",
    "\n",
    "Download reviews dataset (File named Question1) and train wordtoVec using genism on this dataset. \n",
    "You will have to read input file and then preprocess (tokenization, etc) data using nltk (as done in HW1) or genism.\n",
    "Train wordtoVec on this dataset using following command\n",
    "\n",
    "<font color=green> genism.model.Word2Vec(preProcessedData, size, window, min_count, workers) </font>\n",
    "\n",
    "This command will return a trained model which should be saved in some variable for later use. Parameters of this model are as follows:\n",
    "\n",
    "1. <font color=blue> PreProcessedData</font> = This should be list of preprocessed(tokenized) sentences (list of lists)\n",
    "\n",
    "2. <font color=blue> Size</font> = number of dimensions of word vectors\n",
    "\n",
    "3. <font color=blue> Window</font> = number of context words\n",
    "\n",
    "4. <font color=blue> Min_count</font> = words that have frequency lower than min_count are ignored while training\n",
    "\n",
    "5. <font color=blue> Workers</font> = number of threads\n",
    "\n",
    "\n",
    "Use most_similar function<font color=green> (trainedModel.wv.most_similar())</font> of trained model to find most similar words to following words.\n",
    "\n",
    "1. <font color=purple> Clean</font>\n",
    "\n",
    "2. <font color=purple> Unclean</font>\n",
    "\n",
    "3. <font color=purple> Amazed</font>\n",
    "\n",
    "4. <font color=purple> friendly</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets load all the required dependcies.\n",
    "\"\"\"\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(mode):\n",
    "    \"\"\"\n",
    "    It takes only path of folder, no file name.\n",
    "    It only returns the folder which contain all the text file.\n",
    "    \n",
    "    Argument:\n",
    "    mode -- string specifying input or output for directory\n",
    "    \n",
    "    Returns:\n",
    "    dp -- directory path which contains all the txt files.\n",
    "    \"\"\"\n",
    "    if (mode == \"input\"):   \n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw3/Datasets/\"\n",
    "    elif (mode == \"output\"):\n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/hw3/out/\"\n",
    "    else:\n",
    "        raise ValueError('Unspecified mode for I/O.')\n",
    "        dp = None\n",
    "\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_on_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    It takes in path to our dataset and does the preprocessing\n",
    "    on it.\n",
    "    \n",
    "    Argument:\n",
    "    dataset_path -- path to dataset \"dir()+Question1.txt\"\n",
    "    \n",
    "    Returns:\n",
    "    preprocessed_data -- preprocessed list of all words in our dataset\n",
    "    \"\"\"\n",
    "    dataset = open(dataset_path, 'r', encoding = \"utf-8\", errors='ignore')\n",
    "    preprocessed_data = list()\n",
    "    for each_line in dataset:\n",
    "        temp = each_line\n",
    "        preprocessed_words = preprocess_string(temp)\n",
    "#         for w in range(0, len(preprocessed_words)):\n",
    "        preprocessed_data.append(preprocessed_words)\n",
    "                \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = preprocess_string('conceptual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(dpath, model_path):\n",
    "    \"\"\"\n",
    "    It takes in argument as the path of dataset and \n",
    "    another path where we want to store out train model.\n",
    "    \n",
    "    Argument:\n",
    "    dpath -- path to dataset \"dir()+Question1.txt\"\n",
    "    model_path -- path to store our trained model \"dir() + word2vec.model\"\n",
    "    \n",
    "    Returns:\n",
    "    Nothing \n",
    "    \"\"\"\n",
    "    # Call already implemented function: preprocessing_on_dataset()\n",
    "    preprocessed = preprocessing_on_dataset(dpath)\n",
    "    # Begin training using word2vec embeddings\n",
    "    model = Word2Vec(preprocessed, size=100, window=5, min_count=2, workers=4)\n",
    "    # Now, let's save it in any directory path\n",
    "    model.save(model_path)\n",
    "    # return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    It takes in argument as the path of our already stored trained model. \n",
    "    It loads it into a model variable and returns it.\n",
    "    \n",
    "    Argument:\n",
    "    model_path -- path to store our trained model \"dir() + word2vec.model\" \n",
    "    \n",
    "    Returns:\n",
    "    model -- previously trained model on our dataset\n",
    "    \"\"\"\n",
    "    model = Word2Vec.load(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath = get_directory_path('input') + \"Question1.txt\"\n",
    "model_path = get_directory_path('output') + \"word2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dpath, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_trained_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN Most similiar =  [('spotless', 0.737820029258728), ('immacul', 0.6511803865432739), ('tidi', 0.5864765644073486), ('appoint', 0.582635760307312), ('furnish', 0.5426121354103088), ('spaciou', 0.5424081683158875), ('cleanli', 0.520179808139801), ('smallish', 0.5040469169616699), ('fairli', 0.4959147274494171), ('maintain', 0.4947584271430969)] \n",
      "\n",
      "\n",
      "UNCLEAN Most similiar =  [('dirti', 0.7858413457870483), ('grubbi', 0.7467415928840637), ('threadbar', 0.7396768927574158), ('soil', 0.7226435542106628), ('grimi', 0.7210322618484497), ('smelli', 0.7195559144020081), ('dingi', 0.7161514163017273), ('dusti', 0.7156516313552856), ('damp', 0.7155827283859253), ('moldi', 0.7110864520072937)] \n",
      "\n",
      "\n",
      "FRIENDLY Most similiar =  [('freindli', 0.8184008598327637), ('courteou', 0.8091893196105957), ('polit', 0.7867847681045532), ('pleasant', 0.7249651551246643), ('cordial', 0.6901986598968506), ('graciou', 0.6620055437088013), ('friendliest', 0.6548391580581665), ('curteou', 0.6433910131454468), ('friendlier', 0.6153292059898376), ('friendi', 0.6140530109405518)] \n",
      "\n",
      "\n",
      "AMAZED Most similiar =  [('fantast', 0.8762537240982056), ('fabul', 0.8400188684463501), ('awesom', 0.8332536220550537), ('incred', 0.7846707105636597), ('wonder', 0.7779442667961121), ('superb', 0.7339966893196106), ('spectacular', 0.7265146970748901), ('outstand', 0.7181341052055359), ('stun', 0.71065354347229), ('gorgeou', 0.7068237662315369)] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lst = [\"clean\", \"unclean\", \"friendly\", \"Amazed\"]\n",
    "\n",
    "for i in range(0, (len(lst))):\n",
    "    preprocessed_word = preprocess_string(lst[i])\n",
    "    similiar_word_vec = word2vec_trained_model.wv.most_similar(preprocessed_word)\n",
    "    print(lst[i].upper(), \"Most similiar = \", similiar_word_vec, \"\\n\\n\")\n",
    "\n",
    "\n",
    "# clean_most_sim = model.wv.most_similar(\"clean\")\n",
    "# print(clean_most_sim)\n",
    "\n",
    "# unclean_most_sim = model.wv.most_similar(\"unclean\")\n",
    "# print(unclean_most_sim)\n",
    "\n",
    "# friendly_most_sim = model.wv.most_similar(\"friendly\")\n",
    "# print(friendly_most_sim)\n",
    "\n",
    "# amazed_most_sim = model.wv.most_similar(\"Amazed\")\n",
    "# print(amazed_most_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=green> Question 2 (Sentiment Analysis) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment is to build a sentiment classifier using the <font color=red> Naive Bayes classification </font> techniques and a bag of words model.\n",
    "\n",
    "You can use <font color=blue> scikit-learn </font> (machine larning tool for python) for using implementations of classification algorithms.\n",
    "\n",
    "Install scikit using <font color=red>  “pip install scikit-learn” </font>\n",
    "\n",
    "Perform sentiment analysis on attached dataset for question 2. The data set contains movie reviews \n",
    "with labels <font color=red> 0 (negative) </font> and <font color=green> 1 (positive)</font>. \n",
    "\n",
    "Split the data into train and test set by using <font color=red> “train_test_split(DataSet)” </font> of scikit. By default it will split into <font color=brown> 75% </font> training and <font color=brown> 25% </font> test data.\n",
    "\n",
    "Implement following 2 models for sentiment analysis and report accuracies of both models.\n",
    "\n",
    "1. <font color=purple> Bag of words based on raw counts </font>\n",
    "\n",
    "2. <font color=purple> Bag of words based on TfIDF </font>\n",
    "\n",
    "Read following links about using Vectorizer (Bag of words based on raw counts) and transformer \n",
    "(Bag of words based on TfIDF)  for converting list of sentences to vectors\n",
    "\n",
    "1. <font color=blue> https://scikit-learn.org/stable/modules/feature_extraction.html </font>\n",
    "\n",
    "2. <font color=blue> https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py </font>\n",
    "\n",
    "You can use scikit learn for implementation of <font color=red> Naïve Bayes </font> as explained in above links.\n",
    "\n",
    "The data is in <font color=brown> CSV </font> format so you can read it in <font color=brown> dataframe </font> using <font color=blue> pandas </font> library. Use <font color=blue> BeautifulSoup</font> library for parsing html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_q2 = get_directory_path('input') + \"Question2 Dataset.tsv\"\n",
    "ds_tsv = pd.read_csv(dataset_for_q2, sep = '\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_tsv[['id']]\n",
    "# ds_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = ds_tsv.sentiment\n",
    "features = ds_tsv.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print((features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Counts (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target_label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16750,) (8250,) (16750,) (8250,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = vectorizer.fit_transform(X_train)\n",
    "model_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 63263\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: {}\".format(len(vectorizer.vocabulary_)))\n",
    "# print(\"Vocabulary Content:\\n {}\".format(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16750x63263 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1496259 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train\n",
    "\n",
    "# To see the features\n",
    "# vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (first element is training example number or rows of dataset,\n",
    "## second element is index assigned to the word as part of vocabluary)\n",
    "## then is count of text\n",
    "#print(model_train)\n",
    "\n",
    "# import numpy as np\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "array_format = model_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sure\n",
    "# X_train_df = pd.DataFrame(array_format,columns=list(vectorizer.get_feature_names()))\n",
    "# X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((array_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.921\n",
      "Test set score: 0.862\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(model_train,y_train)\n",
    "print(\"Training set score: {:.3f}\".format(nb.score(model_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(nb.score(model_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build TFIDF features on train reviews\n",
    "tfidf_vec = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_features = tfidf_vec.fit_transform(X_train)\n",
    "tfidf_test_features = tfidf_vec.transform(X_test)\n",
    "# tfidf                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sure\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16750, 63263)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_array_format = tfidf_train_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (16750, 63263)  Test features shape: (8250, 63263)\n"
     ]
    }
   ],
   "source": [
    "print('TFIDF model:> Train features shape:', tfidf_train_features.shape, ' Test features shape:', tfidf_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.923\n",
      "Test set score: 0.867\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(tfidf_train_features, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(nb.score(tfidf_train_features,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(nb.score(tfidf_test_features,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nbconvert.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nbconvert.__path__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
