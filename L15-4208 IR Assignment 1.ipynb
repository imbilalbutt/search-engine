{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Bilal Butt.\n",
    "\n",
    "Copyright © 2019 Bilal Butt. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenizing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in creating an index is tokenization. You must convert a document into a stream of tokens suitable for indexing.\n",
    "Your tokenizer should follow these steps:\n",
    "1. Accept a directory name as a command line argument, and process all files found in that directory.\n",
    "2. Extract the document text with an HTML parsing library, ignoring the headers at the beginning of the file and all HTML tags.\n",
    "3. Split the text into tokens (You can use some library for regular expression matching. To learn about regular expressions go to this link http://www.rexegg.com/regex-quickstart.html).\n",
    "4. Convert all tokens to lowercase (this is not always ideal, but indexing intelligently in a case-sensitive manner is tricky).\n",
    "5. Apply stop-wording to the document by ignoring any tokens found in this list <font color=blue>(\\\\Cactus \\xeon\\Maryam Bashir\\Information Retrieval\\stoplist)</font>.\n",
    "6. Apply stemming to the document using any standard algorithm – <font color=green> Porter, Snowball, and KStem stemmers </font> are appropriate. You should use a stemming library for this step.\n",
    "7. Your tokenizer will write two files:\n",
    "- docids.txt – A file mapping a document's filename (without path) to a unique integer, its DOCID. Each line should be formatted with a DOCID and filename separated by a tab, as follows:<font color=blue> 1234\\t32435</font>\n",
    "- termids.txt – A file mapping a token found during tokenization to a unique integer, its TERMID. Each line should be formatted with a TERMID and token separated by a tab, as follows: <font color=blue> 567\\tapple </font>\n",
    "\n",
    "<font color=blue>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "#from sets import Set\n",
    "#from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(mode):\n",
    "    \"\"\"\n",
    "    It takes only path of folder, no file name.\n",
    "    It only returns the folder which contain all the text file.\n",
    "    \n",
    "    Argument:\n",
    "    #nothing\n",
    "    \n",
    "    Returns:\n",
    "    dp -- directory path which contains all the txt files.\n",
    "    \"\"\"\n",
    "    if (mode == \"input\"):   \n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/\"\n",
    "    elif (mode == \"output\"):\n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/out/\"\n",
    "    else:\n",
    "        raise ValueError('Unspecified mode.')\n",
    "\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : read_stop_list\n",
    "def read_text_in_list_form(file_path):\n",
    "    \"\"\"\n",
    "    This function takes the path of stop words file and reads it and returns a list of words.\n",
    "    \n",
    "    Argument:\n",
    "    stop_file_path -- path should be like: path + file name.extension\n",
    "        \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\".\n",
    "    \n",
    "    Returns:\n",
    "    lineList -- list of words containg all the stop_words.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = [line.rstrip('\\n') for line in open(file_path)]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function = remove_html_headers\n",
    "def remove_html_headers(file_path):\n",
    "    \"\"\"\n",
    "    This function takes in path of file + file name and opens it. \n",
    "    Then it remove all the headers of html and returns the plain text.\n",
    "    \n",
    "    Argument:\n",
    "    file_path -- concateted: path folder + file name\n",
    "    \n",
    "    Returns:\n",
    "    soup -- the plain text of str type.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding='utf-8',errors='ignore') as fin:\n",
    "        soup = BeautifulSoup(BeautifulSoup(fin, 'html.parser').prettify()).text\n",
    "        soup = soup.lower()\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: split_text\n",
    "def tokenize_the_text(raw_text):\n",
    "    \"\"\"\n",
    "    This function converts raw text into list[i] i.e into tokens.\n",
    "    \n",
    "    Argument:\n",
    "    raw_text -- class 'str' type.\n",
    "    \n",
    "    Returns:\n",
    "    splitText -- list of unique words extracted from raw_text.\n",
    "    len(splitText) -- length of words in splitText.\n",
    "    \"\"\"\n",
    "    #lower_text = raw_text.lower()\n",
    "    lower_text = raw_text\n",
    "    \"\"\"\n",
    "    # Method 1 : Using regex\n",
    "    \"\"\"\n",
    "    ####splitTextDig = re.split('[a-f]+',lower_text) # comment it\n",
    "    splitTextAlp = str(re.findall(r\"[a-z]+\", lower_text))\n",
    "    \n",
    "    # Uncommenting This line incurrs \"sns_x\" type words \n",
    "    ####splitTextAlp = str(re.split(\" [\\s.,!?:;}{)(>]\",lower_text))\n",
    "    \n",
    "    ####splitTextAlp = str(re.split('[0-9]+', splitTextAlp,flags=re.IGNORECASE)) #comment it\n",
    "    \n",
    "    splitTextAlp = list(re.split(r'\\W+', (splitTextAlp)))\n",
    "#     splitTextAlp = list(re.split(r' ', str(splitTextAlp)))\n",
    "#     splitTextAlp = list(re.split(r'\\W+', str(splitTextAlp)))\n",
    "    # Now, take only those words which make sense and are greater than 3.\n",
    "    splitText = [x for x in splitTextAlp if len(x) > 3]\n",
    "    \n",
    "    # By converting into set it will take only unique words from list.\n",
    "    # Also by type casting converting back to list\n",
    "    splitText = list(set(splitText))\n",
    "    #splitText.sort()\n",
    "    \n",
    "    \"\"\"\n",
    "    #Method 2 : Using ntlk\n",
    "    \"\"\"\n",
    "    #from nltk.tokenize import word_tokenize \n",
    "    #splitTextAlp = word_tokenize(lower_text)\n",
    "    ###splitTextAlp = str(re.findall(r\"[a-z]+\", str(splitTextAlp)))\n",
    "    #splitTextAlp = list(re.split('[0-9]+',str(splitTextAlp)))\n",
    "    #wt = [x for x in word_tokens if len(x) > 3]\n",
    "    #wt = list(set(wt))\n",
    "\n",
    "    \n",
    "    #print(type(splitText))\n",
    "    #splitText = list(set(splitTextAlp).difference(splitTextDig))\n",
    "    return splitText, len(splitText)\n",
    "    #return wt, len(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : remove_stop_words\n",
    "def remove_stop_words(document_words, stop_words):\n",
    "    \"\"\"\n",
    "    This function removes the stop_list from tokens of documents.\n",
    "    Stop_words are those words which occurs in abundance in text.\n",
    "    \n",
    "    Argument:\n",
    "    document_words -- list of all the tokens/words extracted from document.\n",
    "    stop_words -- list of all the stop_list extracted from file.\n",
    "    \n",
    "    Returns:\n",
    "    cleaned -- list of all words which do not have stop_list words.\n",
    "    \"\"\"\n",
    "    #print(type(document_words))\n",
    "    #print(type(stop_words))\n",
    "    cleaned_tokens_from_stop_words =  list(set(document_words) - set(stop_words))\n",
    "    #cleaned_tokens_from_stop_words.sort()\n",
    "    return cleaned_tokens_from_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : stem_words\n",
    "def stem_words(tokenized_words_without_stop_words):\n",
    "    \"\"\"\n",
    "    This function takes in list of words which do not contain stop_words.\n",
    "    It uses the PorterStemmer() to reduce the word to their root words.\n",
    "    \n",
    "    Argument:\n",
    "    removed_all_stop_words -- list of all words which do not have stop_words.\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_words -- list of words which are reduced to their origin word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = list()\n",
    "    for w in tokenized_words_without_stop_words:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    stemmed_words.sort()\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : docid_write_to_file\n",
    "def docid_write_to_file(output_file_name, doc_id, doc_name):\n",
    "    \"\"\"\n",
    "    This function takes in name of output file along with it's directory,\n",
    "    and name of document i.e. name of text file. Then it generates a random \n",
    "    number and then writes random number and text file name to output\n",
    "    file.\n",
    "    \n",
    "    Argument:\n",
    "    output_file_name -- directory path concatenated to output file name.\n",
    "    doc_name -- the name of single text file to write in output file.\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    idx = random.randint(1,5000)\n",
    "    \n",
    "    line = str(doc_id) + \"\\t\" + doc_name\n",
    "           \n",
    "    fh.write(line)\n",
    "    fh.write('\\n')\n",
    "    \n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : termid_write_to_file\n",
    "def termid_write_to_file(output_file_name, all_collection_of_words, all_collection_of_word_ids):\n",
    "    \"\"\"\n",
    "    This function takes in name of output file along with it's directory,\n",
    "    and opens in append mode, and it takes in list of all words found in\n",
    "    all documents in sorted form.\n",
    "    Argument:\n",
    "    output_file_name -- directory path concatenated to output file name.\n",
    "    list_of_all_unique_words -- list of all the unique words in sorted form.\n",
    "    \n",
    "    Returns:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    for i in range(0,len(all_collection_of_words)):\n",
    "        new_line = str(all_collection_of_word_ids[i]) + \"\\t\" + all_collection_of_words[i]\n",
    "        fh.write(new_line)\n",
    "        fh.write('\\n')\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : clear_collection_from_repetition\n",
    "def clear_collection_from_repetition(collection_of_all_words, stemmed_words):\n",
    "    \"\"\"\n",
    "    This function \n",
    "    \n",
    "    Argument:\n",
    "    collection_of_all_words -- .\n",
    "    stemmed_words -- \n",
    "    Returns:\n",
    "    s -- .\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned = list(set(collection_of_all_words) - set(stemmed_words))\n",
    "    common = list(set(collection_of_all_words) & set(stemmed_words))\n",
    "    \n",
    "    for w in range (0,len(stemmed_words)):\n",
    "            collection_of_all_words.append(stemmed_words[w])\n",
    "    #print(cleaned[0])\n",
    "    #cleaned.sort()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : tokenize_the_directory_path\n",
    "def tokenize_the_directory_path(path):\n",
    "    \"\"\"\n",
    "    This function converts directory path into list[i] i.e into tokens.\n",
    "    \n",
    "    Argument:\n",
    "    path -- class 'str' type of path of file.\n",
    "    \n",
    "    Returns:\n",
    "    splitText -- list of unique words extracted from raw_text.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_text = re.split(\"[/]\",path)\n",
    "    \n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_files_in_directory(directory):\n",
    "    # Read all .txt files from directory and save to files list.\n",
    "    files = []\n",
    "    #r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            if '.txt' not in file:\n",
    "                files.append(file)\n",
    "            elif '.txt' not in file:\n",
    "                files.append(file)   \n",
    "            #if '.txt' in file:\n",
    "                #files.append(os.path.join(r, file))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> term_index.txt - </font> An inverted index containing the file position for each occurrence of each term in the collection. Each line should contain the complete inverted list for a single term. Each line should contain a list of DOCID,POSITION values. Each line of this file should contain a TERMID followed by a space-separated list of properties as follows:\n",
    "<font color=green> 347 1542 567 432,43 456,33 456,41 </font>\n",
    "- 347: TERMID\n",
    "- 1542: Total number of occurrences of the term in the entire corpus \n",
    "- 567: Total number of documents in which the term appears\n",
    "- 432: Document Id in which term appears\n",
    "- 43: Position of term in document 432\n",
    "\n",
    "In order to support more efficient compression you must apply delta encoding to the inverted list. The first DOCID for a term and the first POSITION for a document will be stored normally. Subsequent values should be stored as the offset from the prior value.\n",
    "\n",
    "Instead of encoding an inverted list like this: <font color=green>347 1542 567 432,43 456,33 456,41 </font>\n",
    "- You should encode it like this:\n",
    "<font color=green>347 1542 567 432,43 24,33 0,8  </font>\n",
    "- <font color=red> Note that </font> in order to do this, your DOCIDs and POSITIONs must be sorted in ascending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_index_write_to_file(output_file_name, inverted_index_dictionary):\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    for term_id , dicti in inverted_index_dictionary.items():\n",
    "                # term_id \n",
    "        t_id = str(term_id) + \"\\n\"\n",
    "        fh.write(t_id)\n",
    "        for d_id, poss in dicti.items():\n",
    "            total_number_of_occurance = len(poss)\n",
    "                       # doc_id          # count               # positions\n",
    "            lne = \"\\t\"+ str(d_id) +\"\\t\"+ str(len(poss)) +\"\\t\"+ str(poss)\n",
    "            fh.write(lne)\n",
    "            fh.write('\\n')\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postings_for_unhashed_ii(list_of_doc_and_terms):\n",
    "    my_dict = dict()\n",
    "    for tup in list_of_doc_and_terms:\n",
    "        temp = list()\n",
    "        key = 0\n",
    "        i = 1\n",
    "        for element in tup:\n",
    "            if(np.mod(i,2) != 0): # value = doc_id\n",
    "                if (my_dict.get(key,0) == 0):\n",
    "                    temp.append(element)\n",
    "                else:\n",
    "                    history = my_dict[key]\n",
    "                    for e in range(0, len(history)):\n",
    "                        temp.append(history[e])\n",
    "                    temp.append(element)\n",
    "            elif(np.mod(i,2) == 0): # key = term_id\n",
    "                key = element\n",
    "            i+=1\n",
    "        my_dict[key] = list(set(temp))\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashed Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build version 4.0 #dictionary within dictionary #WORKING\n",
    "def create_hashed_inverted_index():\n",
    "    # Get directory where all the .txt files are present.\n",
    "    directory = get_directory_path(\"input\")\n",
    "    output_directory = get_directory_path(\"output\")\n",
    "    \n",
    "    extension = \".txt\"\n",
    "    document_output_file_name = output_directory + \"docid_hashed\" + extension\n",
    "    term_output_file_name = output_directory + \"termid_hashed\" + extension\n",
    "    \n",
    "    term_index_dot_txt = get_directory_path(\"output\") + \"term_index_hashed\" + extension\n",
    "    \n",
    "    # Load all stop words into a stop_list.\n",
    "    stop_list_path = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\"\n",
    "    stop_list = read_text_in_list_form(stop_list_path)\n",
    "    \n",
    "    files = find_all_files_in_directory(directory)\n",
    "    hashed_ii = dict() ## (key:terms_id, )\n",
    "    all_collection_of_words = list()\n",
    "    all_collection_of_word_ids = list()\n",
    "    dict_for_docs = dict() ## (key: doc_id, value:terms_found_in_this_doc)\n",
    "    history = dict()\n",
    "    \n",
    "    ## One doc file is opened.\n",
    "    for i in range(0,int(len(files))):\n",
    "        file_path = directory + files[i] #+ extension # no need for extension in both cases of file\n",
    "        #print(file_path)\n",
    "        all_text_without_html_headers = remove_html_headers(file_path) #str\n",
    "        tokenized_words, count_of_words = tokenize_the_text(all_text_without_html_headers) #list\n",
    "        tokenized_words_without_stop_words = remove_stop_words(tokenized_words, stop_list) #list\n",
    "        stemmed_tokens_of_words = stem_words(tokenized_words_without_stop_words) #list \n",
    "\n",
    "        doc_id = random.randint(1,8000)\n",
    "        doc_and_position_container = dict()\n",
    "        ## Append all words of a doc to list\n",
    "        for w in range(0,len(stemmed_tokens_of_words)):     \n",
    "            current_word = stemmed_tokens_of_words[w]\n",
    "            term_id = random.randint(1,100000)\n",
    "            while term_id in hashed_ii.keys():\n",
    "                term_id = random.randint(1,100000)\n",
    "                \n",
    "            ## If that word already present\n",
    "            if (all_collection_of_words.count(current_word)) != 0:\n",
    "                first_occurance = all_collection_of_words.index(current_word)\n",
    "                term_id = all_collection_of_word_ids[first_occurance]\n",
    "                history = dict()\n",
    "                history = hashed_ii[str(term_id)] \n",
    "                \n",
    "                for d_id, positions_list in history.items():\n",
    "                    if d_id == str(doc_id):\n",
    "                        already_existed_positions = list()\n",
    "                        already_existed_positions = positions_list #(history[d_id])\n",
    "                        already_existed_positions.append(stemmed_tokens_of_words.index(current_word,w,len(stemmed_tokens_of_words)))\n",
    "                        #print(d_id, term_id, current_word)\n",
    "                        history.update({d_id : already_existed_positions})\n",
    "                        hashed_ii[str(term_id)] = history\n",
    "            ## If already not present that word\n",
    "            else:\n",
    "                all_collection_of_words.append(current_word)\n",
    "                all_collection_of_word_ids.append(str(term_id))\n",
    "                lsr = list()\n",
    "                position = stemmed_tokens_of_words.index(current_word) \n",
    "                lsr.append(position)\n",
    "                doc_and_position_container = dict()\n",
    "                doc_and_position_container[str(doc_id)] = lsr\n",
    "                hashed_ii[str(term_id)] = dict()\n",
    "                hashed_ii.update({str(term_id) : doc_and_position_container})\n",
    "        docid_write_to_file(document_output_file_name,doc_id,files[i])\n",
    "    #print(\"hashed_ii = \", hashed_ii)\n",
    "    termid_write_to_file(term_output_file_name, all_collection_of_words, all_collection_of_word_ids)\n",
    "    term_index_write_to_file(term_index_dot_txt, hashed_ii)\n",
    "    \n",
    "    print(\"Happy Ending\")\n",
    "    print(len(all_collection_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create hashed inverted index execute following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2648 47777 concert\n",
      "2648 38064 http\n",
      "2648 24060 music\n",
      "2648 24060 music\n",
      "2648 48344 publish\n",
      "2648 59274 record\n",
      "2648 84144 show\n",
      "2648 93495 song\n",
      "2648 93672 templat\n",
      "2648 48625 ticket\n",
      "2648 84029 william\n",
      "6083 41351 categori\n",
      "6083 39561 chang\n",
      "6083 94321 classic\n",
      "6083 29539 contain\n",
      "6083 8286 cooki\n",
      "6083 68014 flag\n",
      "6083 7958 icon\n",
      "6083 37225 imag\n",
      "6083 88139 nation\n",
      "6083 73811 open\n",
      "6083 26000 plugin\n",
      "6083 22886 shop\n",
      "6083 22886 shop\n",
      "6083 84800 state\n",
      "6083 11872 theme\n",
      "6083 34948 widget\n",
      "7544 54684 delici\n",
      "7544 87585 flavor\n",
      "7544 49028 guarante\n",
      "7544 94714 help\n",
      "7544 52937 item\n",
      "7544 2706 scientif\n",
      "7544 56685 translat\n",
      "7544 19354 underscor\n",
      "854 50285 america\n",
      "854 77347 balanc\n",
      "854 4160 candid\n",
      "854 18902 colleg\n",
      "854 44046 edit\n",
      "854 3751 elect\n",
      "854 3751 elect\n",
      "854 28002 elector\n",
      "854 28002 elector\n",
      "854 54348 forum\n",
      "854 57914 guest\n",
      "854 95609 happen\n",
      "854 56807 linkback\n",
      "854 58475 make\n",
      "854 19746 pingback\n",
      "854 10001 popul\n",
      "854 35813 regist\n",
      "854 27524 repli\n",
      "854 65439 select\n",
      "854 38346 shift\n",
      "854 93770 thousand\n",
      "854 91727 thread\n",
      "854 91727 thread\n",
      "854 79507 trust\n",
      "854 61110 understand\n",
      "854 2822 vote\n",
      "854 2822 vote\n",
      "854 2822 vote\n",
      "854 42605 win\n",
      "854 68411 write\n",
      "5370 77409 dream\n",
      "5370 10505 lectur\n",
      "5370 44577 lesson\n",
      "4667 15578 album\n",
      "4667 32639 artist\n",
      "4667 67145 bitch\n",
      "4667 77636 friend\n",
      "4667 85612 homi\n",
      "4667 94769 hustler\n",
      "4667 45277 nigga\n",
      "4667 36958 rington\n",
      "4667 75131 stori\n",
      "4667 77415 temptat\n",
      "4667 71030 thug\n",
      "5427 6678 activ\n",
      "5427 19836 advertis\n",
      "5427 19836 advertis\n",
      "5427 93968 affili\n",
      "5427 62452 aggreg\n",
      "5427 6786 appli\n",
      "5427 63888 appoint\n",
      "5427 17414 assist\n",
      "5427 98830 board\n",
      "5427 5862 brand\n",
      "5427 36845 collect\n",
      "5427 36845 collect\n",
      "5427 60241 commerci\n",
      "5427 5813 comput\n",
      "5427 52244 configur\n",
      "5427 24652 consent\n",
      "5427 24652 consent\n",
      "5427 27498 dealer\n",
      "5427 3230 determin\n",
      "5427 94952 disclos\n",
      "5427 94952 disclos\n",
      "5427 94952 disclos\n",
      "5427 75042 disclosur\n",
      "5427 484 effort\n",
      "5427 31681 email\n",
      "5427 26687 enhanc\n",
      "5427 55813 entiti\n",
      "5427 40213 entitl\n",
      "5427 9345 financ\n",
      "5427 56777 identifi\n",
      "5427 56777 identifi\n",
      "5427 39800 independ\n",
      "5427 63114 individu\n",
      "5427 2066 logo\n",
      "5427 27592 maintain\n",
      "5427 27592 maintain\n",
      "5427 63051 manag\n",
      "5427 4353 manufactur\n",
      "5427 28054 materi\n",
      "5427 919 measur\n",
      "5427 63457 network\n",
      "5427 69327 notifi\n",
      "5427 17763 obtain\n",
      "5427 78993 offer\n",
      "5427 41071 opt\n",
      "5427 30924 own\n",
      "5427 74350 permit\n",
      "5427 20981 possibl\n",
      "5427 51563 promot\n",
      "5427 51563 promot\n",
      "5427 90652 protect\n",
      "5427 78113 read\n",
      "5427 38902 reason\n",
      "5427 3807 reject\n",
      "5427 26500 remov\n",
      "5427 6989 secur\n",
      "5427 57432 solicit\n",
      "5427 82832 specif\n",
      "5427 53272 statist\n",
      "5427 32961 subpoena\n",
      "5427 25667 trademark\n",
      "5427 63855 unsubscrib\n",
      "5427 23018 vehicl\n",
      "5427 6510 visitor\n",
      "4015 75496 angel\n",
      "4015 55073 appear\n",
      "4015 85459 cage\n",
      "4015 66271 comic\n",
      "4015 33283 nerd\n",
      "4015 86510 plan\n",
      "4015 79760 play\n",
      "4015 62465 say\n",
      "4015 90001 steal\n",
      "4196 55227 averag\n",
      "4196 44889 poll\n",
      "4196 44889 poll\n",
      "7813 11190 danger\n",
      "7813 67178 neighborhood\n",
      "7813 95528 sorcer\n",
      "7813 21905 voic\n",
      "2927 65991 activist\n",
      "2927 79860 advoc\n",
      "2927 79860 advoc\n",
      "2927 17772 ask\n",
      "2927 18260 attend\n",
      "2927 18260 attend\n",
      "2927 75350 centr\n",
      "2927 79793 chair\n",
      "2927 62775 challeng\n",
      "2927 40040 coalit\n",
      "2927 36845 collect\n",
      "2927 36845 collect\n",
      "2927 2373 constitut\n",
      "2927 43993 council\n",
      "2927 88814 deliv\n",
      "2927 21631 demand\n",
      "2927 21631 demand\n",
      "2927 4697 difficulti\n",
      "2927 25816 district\n",
      "2927 85088 economi\n",
      "2927 20489 feder\n",
      "2927 83665 fund\n",
      "2927 91623 institut\n",
      "2927 35954 labor\n",
      "2927 11145 leon\n",
      "2927 59402 meet\n",
      "2927 43958 minist\n",
      "2927 85646 offici\n",
      "2927 9339 organis\n",
      "2927 20508 pencil\n",
      "2927 87936 politician\n",
      "2927 88338 promis\n",
      "2927 20896 slate\n",
      "2927 54037 studi\n",
      "2927 74279 union\n",
      "2927 53947 weight\n",
      "3459 38064 http\n",
      "3459 38064 http\n",
      "6244 74699 airport\n",
      "6244 39473 deal\n",
      "6244 10132 hotel\n",
      "782 58856 shirt\n",
      "Happy Ending\n",
      "3776\n"
     ]
    }
   ],
   "source": [
    "create_hashed_inverted_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unhashed Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unhashed_inverted_index():\n",
    "    # Get directory where all the .txt files are present.\n",
    "    directory = get_directory_path(\"input\")\n",
    "    output_directory = get_directory_path(\"output\")\n",
    "    \n",
    "    extension = \".txt\"\n",
    "    document_output_file_name = output_directory + \"docid_unhashed\" + extension\n",
    "    term_output_file_name = output_directory + \"termid_unhashed\" + extension\n",
    "    \n",
    "    term_index_dot_txt = get_directory_path(\"output\") + \"term_index_unhashed\" + extension\n",
    "    \n",
    "    # Load all stop words into a stop_list.\n",
    "    stop_list_path = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\"\n",
    "    stop_list = read_text_in_list_form(stop_list_path)\n",
    "    \n",
    "    files = find_all_files_in_directory(directory)\n",
    "    \n",
    "    list_of_docs = list() # for storing the ids of doc\n",
    "    list_of_term_ids = list() # for storing the ids of all terms\n",
    "    list_of_doc_and_terms = list() # for storing as tuples\n",
    "    all_collection_of_words = list() # for storing all the unique words\n",
    "    all_collection_of_word_ids = list() # for storing all the unique ids\n",
    "    \n",
    "    for i in range(0,len(files)):\n",
    "        file_path = directory + files[i] #+ extension # no need for extension in both cases of file\n",
    "        all_text_without_html_headers = remove_html_headers(file_path) #str\n",
    "        tokenized_words, count_of_words = tokenize_the_text(all_text_without_html_headers) #list\n",
    "        tokenized_words_without_stop_words = remove_stop_words(tokenized_words, stop_list) #list\n",
    "        stemmed_tokens_of_words = stem_words(tokenized_words_without_stop_words) #list \n",
    "        \n",
    "        doc_id = random.randint(1,8000)\n",
    "        \n",
    "        word_id_postings_list = list()\n",
    "        ## Append all words of a doc to list\n",
    "        for w in range(0,len(stemmed_tokens_of_words)):     \n",
    "            current_word = stemmed_tokens_of_words[w]\n",
    "            \n",
    "            term_id = random.randint(1,100000)\n",
    "            while term_id in all_collection_of_word_ids:\n",
    "                term_id = random.randint(1,100000)\n",
    "            \n",
    "            # if that word is not new, already present\n",
    "            if (all_collection_of_words.count(current_word)) != 0:\n",
    "                first_occurance = all_collection_of_words.index(current_word)\n",
    "                term_id = all_collection_of_word_ids[first_occurance]\n",
    "                #tupleX = (doc_id, str(term_id))\n",
    "                #list_of_doc_and_terms.append(tupleX)\n",
    "                list_of_docs.append(doc_id)\n",
    "                list_of_term_ids.append(term_id)\n",
    "            else:\n",
    "                #list_of_term_ids.append(term_id)\n",
    "                #tupleX = (doc_id, str(term_id))\n",
    "                #list_of_doc_and_terms.append(tupleX[1])\n",
    "                all_collection_of_words.append(current_word)\n",
    "                all_collection_of_word_ids.append((term_id))\n",
    "                list_of_docs.append(doc_id)\n",
    "                list_of_term_ids.append(term_id)\n",
    "            \n",
    "        #list_of_docs.append(doc_id)\n",
    "        docid_write_to_file(document_output_file_name,doc_id,files[i])\n",
    "        # sort on the basis of term_ids\n",
    "    #list_of_doc_and_terms.sort(key = operator.itemgetter(1))\n",
    "    \n",
    "    list_of_doc_and_terms = list(zip(list_of_docs, list_of_term_ids))\n",
    "    list_of_doc_and_terms.sort(key = operator.itemgetter(1))\n",
    "#     print((list_of_doc_and_terms))\n",
    "\n",
    "    dict_for_termid_and_postings = create_postings_for_unhashed_ii(list_of_doc_and_terms)\n",
    "    termid_write_to_file(term_output_file_name, all_collection_of_words, all_collection_of_word_ids)\n",
    "    \n",
    "    term_index_write_to_file(term_index_dot_txt, dict_for_termid_and_postings)\n",
    "    \n",
    "    print(\"Happy Ending\")\n",
    "    print(len(all_collection_of_words))\n",
    "#    print(dict_for_termid_and_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create hashed inverted index execute following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Ending\n",
      "8781\n"
     ]
    }
   ],
   "source": [
    "create_unhashed_inverted_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
