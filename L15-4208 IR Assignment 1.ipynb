{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Tokenizing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in creating an index is tokenization. You must convert a document into a stream of tokens suitable for indexing.\n",
    "Your tokenizer should follow these steps:\n",
    "1. Accept a directory name as a command line argument, and process all files found in that directory.\n",
    "2. Extract the document text with an HTML parsing library, ignoring the headers at the beginning of the file and all HTML tags.\n",
    "3. Split the text into tokens (You can use some library for regular expression matching. To learn about regular expressions go to this link http://www.rexegg.com/regex-quickstart.html).\n",
    "4. Convert all tokens to lowercase (this is not always ideal, but indexing intelligently in a case-sensitive manner is tricky).\n",
    "5. Apply stop-wording to the document by ignoring any tokens found in this list <font color=blue>(\\\\Cactus \\xeon\\Maryam Bashir\\Information Retrieval\\stoplist)</font>.\n",
    "6. Apply stemming to the document using any standard algorithm – <font color=green> Porter, Snowball, and KStem stemmers </font> are appropriate. You should use a stemming library for this step.\n",
    "7. Your tokenizer will write two files:\n",
    "- docids.txt – A file mapping a document's filename (without path) to a unique integer, its DOCID. Each line should be formatted with a DOCID and filename separated by a tab, as follows:<font color=blue> 1234\\t32435</font>\n",
    "- termids.txt – A file mapping a token found during tokenization to a unique integer, its TERMID. Each line should be formatted with a TERMID and token separated by a tab, as follows: <font color=blue> 567\\tapple </font>\n",
    "\n",
    "<font color=blue>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "import numpy as np\n",
    "#from sets import Set\n",
    "#from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_path(mode):\n",
    "    \"\"\"\n",
    "    It takes only path of folder, no file name.\n",
    "    It only returns the folder which contain all the text file.\n",
    "    \n",
    "    Argument:\n",
    "    #nothing\n",
    "    \n",
    "    Returns:\n",
    "    dp -- directory path which contains all the txt files.\n",
    "    \"\"\"\n",
    "    if (mode == \"input\"):   \n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/\"\n",
    "    elif (mode == \"output\"):\n",
    "        dp = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/out/\"\n",
    "    else:\n",
    "        raise ValueError('Unspecified mode.')\n",
    "\n",
    "    return dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : read_stop_list\n",
    "def read_text_in_list_form(file_path):\n",
    "    \"\"\"\n",
    "    This function takes the path of stop words file and reads it and returns a list of words.\n",
    "    \n",
    "    Argument:\n",
    "    stop_file_path -- path should be like: path + file name.extension\n",
    "        \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\".\n",
    "    \n",
    "    Returns:\n",
    "    lineList -- list of words containg all the stop_words.\n",
    "    \"\"\"\n",
    "    \n",
    "    lst = [line.rstrip('\\n') for line in open(file_path)]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function = remove_html_headers\n",
    "def remove_html_headers(file_path):\n",
    "    \"\"\"\n",
    "    This function takes in path of file + file name and opens it. \n",
    "    Then it remove all the headers of html and returns the plain text.\n",
    "    \n",
    "    Argument:\n",
    "    file_path -- concateted: path folder + file name\n",
    "    \n",
    "    Returns:\n",
    "    soup -- the plain text of str type.\n",
    "    \"\"\"\n",
    "    with open(file_path, encoding='utf-8',errors='ignore') as fin:\n",
    "        soup = BeautifulSoup(BeautifulSoup(fin, 'html.parser').prettify()).text\n",
    "        soup = soup.lower()\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: split_text\n",
    "def tokenize_the_text(raw_text):\n",
    "    \"\"\"\n",
    "    This function converts raw text into list[i] i.e into tokens.\n",
    "    \n",
    "    Argument:\n",
    "    raw_text -- class 'str' type.\n",
    "    \n",
    "    Returns:\n",
    "    splitText -- list of unique words extracted from raw_text.\n",
    "    len(splitText) -- length of words in splitText.\n",
    "    \"\"\"\n",
    "    #lower_text = raw_text.lower()\n",
    "    lower_text = raw_text\n",
    "    \"\"\"\n",
    "    # Method 1 : Using regex\n",
    "    \"\"\"\n",
    "    ####splitTextDig = re.split('[a-f]+',lower_text) # comment it\n",
    "    splitTextAlp = str(re.findall(r\"[a-z]+\", lower_text))\n",
    "    \n",
    "    # Uncommenting This line incurrs \"sns_x\" type words \n",
    "    ####splitTextAlp = str(re.split(\" [\\s.,!?:;}{)(>]\",lower_text))\n",
    "    \n",
    "    ####splitTextAlp = str(re.split('[0-9]+', splitTextAlp,flags=re.IGNORECASE)) #comment it\n",
    "    \n",
    "    splitTextAlp = list(re.split(r'\\W+', (splitTextAlp)))\n",
    "#     splitTextAlp = list(re.split(r' ', str(splitTextAlp)))\n",
    "#     splitTextAlp = list(re.split(r'\\W+', str(splitTextAlp)))\n",
    "    # Now, take only those words which make sense and are greater than 3.\n",
    "    splitText = [x for x in splitTextAlp if len(x) > 3]\n",
    "    \n",
    "    # By converting into set it will take only unique words from list.\n",
    "    # Also by type casting converting back to list\n",
    "    splitText = list(set(splitText))\n",
    "    #splitText.sort()\n",
    "    \n",
    "    \"\"\"\n",
    "    #Method 2 : Using ntlk\n",
    "    \"\"\"\n",
    "    #from nltk.tokenize import word_tokenize \n",
    "    #splitTextAlp = word_tokenize(lower_text)\n",
    "    ###splitTextAlp = str(re.findall(r\"[a-z]+\", str(splitTextAlp)))\n",
    "    #splitTextAlp = list(re.split('[0-9]+',str(splitTextAlp)))\n",
    "    #wt = [x for x in word_tokens if len(x) > 3]\n",
    "    #wt = list(set(wt))\n",
    "\n",
    "    \n",
    "    #print(type(splitText))\n",
    "    #splitText = list(set(splitTextAlp).difference(splitTextDig))\n",
    "    return splitText, len(splitText)\n",
    "    #return wt, len(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : remove_stop_words\n",
    "def remove_stop_words(document_words, stop_words):\n",
    "    \"\"\"\n",
    "    This function removes the stop_list from tokens of documents.\n",
    "    Stop_words are those words which occurs in abundance in text.\n",
    "    \n",
    "    Argument:\n",
    "    document_words -- list of all the tokens/words extracted from document.\n",
    "    stop_words -- list of all the stop_list extracted from file.\n",
    "    \n",
    "    Returns:\n",
    "    cleaned -- list of all words which do not have stop_list words.\n",
    "    \"\"\"\n",
    "    #print(type(document_words))\n",
    "    #print(type(stop_words))\n",
    "    cleaned_tokens_from_stop_words =  list(set(document_words) - set(stop_words))\n",
    "    #cleaned_tokens_from_stop_words.sort()\n",
    "    return cleaned_tokens_from_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : stem_words\n",
    "def stem_words(tokenized_words_without_stop_words):\n",
    "    \"\"\"\n",
    "    This function takes in list of words which do not contain stop_words.\n",
    "    It uses the PorterStemmer() to reduce the word to their root words.\n",
    "    \n",
    "    Argument:\n",
    "    removed_all_stop_words -- list of all words which do not have stop_words.\n",
    "    \n",
    "    Returns:\n",
    "    stemmed_words -- list of words which are reduced to their origin word.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = list()\n",
    "    for w in tokenized_words_without_stop_words:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    stemmed_words.sort()\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : docid_write_to_file\n",
    "def docid_write_to_file(output_file_name, doc_id, doc_name):\n",
    "    \"\"\"\n",
    "    This function takes in name of output file along with it's directory,\n",
    "    and name of document i.e. name of text file. Then it generates a random \n",
    "    number and then writes random number and text file name to output\n",
    "    file.\n",
    "    \n",
    "    Argument:\n",
    "    output_file_name -- directory path concatenated to output file name.\n",
    "    doc_name -- the name of single text file to write in output file.\n",
    "    \n",
    "    Returns:\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    idx = random.randint(1,5000)\n",
    "    \n",
    "    line = str(doc_id) + \"\\t\" + doc_name\n",
    "           \n",
    "    fh.write(line)\n",
    "    fh.write('\\n')\n",
    "    \n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : termid_write_to_file\n",
    "def termid_write_to_file(output_file_name, all_collection_of_words, all_collection_of_word_ids):\n",
    "    \"\"\"\n",
    "    This function takes in name of output file along with it's directory,\n",
    "    and opens in append mode, and it takes in list of all words found in\n",
    "    all documents in sorted form.\n",
    "    Argument:\n",
    "    output_file_name -- directory path concatenated to output file name.\n",
    "    list_of_all_unique_words -- list of all the unique words in sorted form.\n",
    "    \n",
    "    Returns:\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    for i in range(0,len(all_collection_of_words)):\n",
    "        new_line = str(all_collection_of_word_ids[i]) + \"\\t\" + all_collection_of_words[i]\n",
    "        fh.write(new_line)\n",
    "        fh.write('\\n')\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : clear_collection_from_repetition\n",
    "def clear_collection_from_repetition(collection_of_all_words, stemmed_words):\n",
    "    \"\"\"\n",
    "    This function \n",
    "    \n",
    "    Argument:\n",
    "    collection_of_all_words -- .\n",
    "    stemmed_words -- \n",
    "    Returns:\n",
    "    s -- .\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned = list(set(collection_of_all_words) - set(stemmed_words))\n",
    "    common = list(set(collection_of_all_words) & set(stemmed_words))\n",
    "    \n",
    "    for w in range (0,len(stemmed_words)):\n",
    "            collection_of_all_words.append(stemmed_words[w])\n",
    "    #print(cleaned[0])\n",
    "    #cleaned.sort()\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function : tokenize_the_directory_path\n",
    "def tokenize_the_directory_path(path):\n",
    "    \"\"\"\n",
    "    This function converts directory path into list[i] i.e into tokens.\n",
    "    \n",
    "    Argument:\n",
    "    path -- class 'str' type of path of file.\n",
    "    \n",
    "    Returns:\n",
    "    splitText -- list of unique words extracted from raw_text.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_text = re.split(\"[/]\",path)\n",
    "    \n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_files_in_directory(directory):\n",
    "    # Read all .txt files from directory and save to files list.\n",
    "    files = []\n",
    "    #r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(directory):\n",
    "        for file in f:\n",
    "            if '.txt' not in file:\n",
    "            #if '.txt' in file:\n",
    "                #files.append(os.path.join(r, file))\n",
    "                files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> term_index.txt - </font> An inverted index containing the file position for each occurrence of each term in the collection. Each line should contain the complete inverted list for a single term. Each line should contain a list of DOCID,POSITION values. Each line of this file should contain a TERMID followed by a space-separated list of properties as follows:\n",
    "<font color=green> 347 1542 567 432,43 456,33 456,41 </font>\n",
    "- 347: TERMID\n",
    "- 1542: Total number of occurrences of the term in the entire corpus \n",
    "- 567: Total number of documents in which the term appears\n",
    "- 432: Document Id in which term appears\n",
    "- 43: Position of term in document 432\n",
    "\n",
    "In order to support more efficient compression you must apply delta encoding to the inverted list. The first DOCID for a term and the first POSITION for a document will be stored normally. Subsequent values should be stored as the offset from the prior value.\n",
    "\n",
    "Instead of encoding an inverted list like this: <font color=green>347 1542 567 432,43 456,33 456,41 </font>\n",
    "- You should encode it like this:\n",
    "<font color=green>347 1542 567 432,43 24,33 0,8  </font>\n",
    "- <font color=red> Note that </font> in order to do this, your DOCIDs and POSITIONs must be sorted in ascending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_index_write_to_file(output_file_name, inverted_index_dictionary):\n",
    "    fh = open(output_file_name, \"a+\")\n",
    "    for k , v in inverted_index_dictionary.items():\n",
    "        total_number_of_occurance = len(v)\n",
    "        #     term_id.      # it's total count.                     # all doc_id or postings\n",
    "        lne = str(k) + \"\\t\" + str(len(v)) + \"\\t\" + str(v)\n",
    "        fh.write(lne)\n",
    "        fh.write('\\n')\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postings_for_unhashed_ii(list_of_doc_and_terms):\n",
    "    my_dict = dict()\n",
    "    for tup in list_of_doc_and_terms:\n",
    "        temp = list()\n",
    "        key = 0\n",
    "        i = 1\n",
    "        for element in tup:\n",
    "            if(np.mod(i,2) != 0): # value = doc_id\n",
    "                if (my_dict.get(key,0) == 0):\n",
    "                    temp.append(element)\n",
    "                else:\n",
    "                    history = my_dict[key]\n",
    "                    for e in range(0, len(history)):\n",
    "                        temp.append(history[e])\n",
    "                    temp.append(element)\n",
    "            elif(np.mod(i,2) == 0): # key = term_id\n",
    "                key = element\n",
    "            i+=1\n",
    "        my_dict[key] = list(set(temp))\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashed Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build version 2.0\n",
    "def create_hashed_inverted_index():\n",
    "    # Get directory where all the .txt files are present.\n",
    "    directory = get_directory_path(\"input\")\n",
    "    output_directory = get_directory_path(\"output\")\n",
    "    \n",
    "    extension = \".txt\"\n",
    "    document_output_file_name = output_directory + \"docid_hashed\" + extension\n",
    "    term_output_file_name = output_directory + \"termid_hashed\" + extension\n",
    "    \n",
    "    term_index_dot_txt = get_directory_path(\"output\") + \"term_index_hashed\" + extension\n",
    "    \n",
    "    # Load all stop words into a stop_list.\n",
    "    stop_list_path = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\"\n",
    "    stop_list = read_text_in_list_form(stop_list_path)\n",
    "    \n",
    "    files = find_all_files_in_directory(directory)\n",
    "    hashed_ii = dict() ## (key:terms_id, )\n",
    "    all_collection_of_words = list()\n",
    "    all_collection_of_word_ids = list()\n",
    "    dict_for_docs = dict() ## (key: doc_id, value:terms_found_in_this_doc)\n",
    "    \n",
    "    ## One doc file is opened.\n",
    "    for i in range(0,int(len(files)/500)):\n",
    "        file_path = directory + files[i] #+ extension # no need for extension in both cases of file\n",
    "        print(file_path)\n",
    "        all_text_without_html_headers = remove_html_headers(file_path) #str\n",
    "        tokenized_words, count_of_words = tokenize_the_text(all_text_without_html_headers) #list\n",
    "        tokenized_words_without_stop_words = remove_stop_words(tokenized_words, stop_list) #list\n",
    "        stemmed_tokens_of_words = stem_words(tokenized_words_without_stop_words) #list \n",
    "        \n",
    "        doc_id = random.randint(1,8000)\n",
    "        \n",
    "        word_id_postings_list = list()\n",
    "        ## Append all words of a doc to list\n",
    "        for w in range(0,len(stemmed_tokens_of_words)):     \n",
    "            current_word = stemmed_tokens_of_words[w]\n",
    "            \n",
    "            term_id = random.randint(1,100000)\n",
    "            while term_id in hashed_ii.keys():\n",
    "                term_id = random.randint(1,100000)\n",
    "                \n",
    "            ## If that word already present\n",
    "            if (all_collection_of_words.count(current_word)) != 0:\n",
    "                first_occurance = all_collection_of_words.index(current_word)\n",
    "                term_id = all_collection_of_word_ids[first_occurance]\n",
    "                \n",
    "                lst = list()\n",
    "                history = hashed_ii[term_id]\n",
    "                \n",
    "                for e in range(0, len(history)):\n",
    "                    lst.append(history[e])\n",
    "                lst.append(doc_id)\n",
    "                \n",
    "                lst = list(set(lst))\n",
    "                hashed_ii.update({term_id :lst})\n",
    "            else:\n",
    "                ## already not present that word\n",
    "                ## new place created in dictionary\n",
    "                lst = list()\n",
    "                lst.append((doc_id))\n",
    "                hashed_ii[str(term_id)] = lst\n",
    "                \n",
    "                all_collection_of_words.append(current_word)\n",
    "                all_collection_of_word_ids.append(str(term_id))\n",
    "        docid_write_to_file(document_output_file_name,doc_id,files[i])\n",
    "    \n",
    "    termid_write_to_file(term_output_file_name, all_collection_of_words, all_collection_of_word_ids)\n",
    "    term_index_write_to_file(term_index_dot_txt, hashed_ii)\n",
    "    \n",
    "    print(\"Happy Ending\")\n",
    "    print(len(all_collection_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create hashed inverted index execute following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-1202wb-26-10513\n",
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-1102wb-73-18046\n",
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-1505wb-68-30103\n",
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-0303wb-53-27200\n",
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-1905wb-44-08158\n",
      "/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/corpus/corpus/clueweb12-1012wb-63-19337\n",
      "Happy Ending\n",
      "1738\n"
     ]
    }
   ],
   "source": [
    "create_hashed_inverted_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unhashed Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unhashed_inverted_index():\n",
    "    # Get directory where all the .txt files are present.\n",
    "    directory = get_directory_path(\"input\")\n",
    "    output_directory = get_directory_path(\"output\")\n",
    "    \n",
    "    extension = \".txt\"\n",
    "    document_output_file_name = output_directory + \"docid_unhashed\" + extension\n",
    "    term_output_file_name = output_directory + \"termid_unhashed\" + extension\n",
    "    \n",
    "    term_index_dot_txt = get_directory_path(\"output\") + \"term_index_unhashed\" + extension\n",
    "    \n",
    "    # Load all stop words into a stop_list.\n",
    "    stop_list_path = \"/Users/imbilalbutt/Documents/Semesters/Semester 9/Information Retrieval/Assignment/stoplist.txt\"\n",
    "    stop_list = read_text_in_list_form(stop_list_path)\n",
    "    \n",
    "    files = find_all_files_in_directory(directory)\n",
    "    \n",
    "    list_of_docs = list() # for storing the ids of doc\n",
    "    list_of_term_ids = list() # for storing the ids of all terms\n",
    "    list_of_doc_and_terms = list() # for storing as tuples\n",
    "    all_collection_of_words = list() # for storing all the unique words\n",
    "    all_collection_of_word_ids = list() # for storing all the unique ids\n",
    "    \n",
    "    for i in range(0,len(files)):\n",
    "        file_path = directory + files[i] #+ extension # no need for extension in both cases of file\n",
    "        all_text_without_html_headers = remove_html_headers(file_path) #str\n",
    "        tokenized_words, count_of_words = tokenize_the_text(all_text_without_html_headers) #list\n",
    "        tokenized_words_without_stop_words = remove_stop_words(tokenized_words, stop_list) #list\n",
    "        stemmed_tokens_of_words = stem_words(tokenized_words_without_stop_words) #list \n",
    "        \n",
    "        doc_id = random.randint(1,8000)\n",
    "        \n",
    "        word_id_postings_list = list()\n",
    "        ## Append all words of a doc to list\n",
    "        for w in range(0,len(stemmed_tokens_of_words)):     \n",
    "            current_word = stemmed_tokens_of_words[w]\n",
    "            \n",
    "            term_id = random.randint(1,100000)\n",
    "            while term_id in all_collection_of_word_ids:\n",
    "                term_id = random.randint(1,100000)\n",
    "            \n",
    "            # if that word is not new, already present\n",
    "            if (all_collection_of_words.count(current_word)) != 0:\n",
    "                first_occurance = all_collection_of_words.index(current_word)\n",
    "                term_id = all_collection_of_word_ids[first_occurance]\n",
    "                #tupleX = (doc_id, str(term_id))\n",
    "                #list_of_doc_and_terms.append(tupleX)\n",
    "                list_of_docs.append(doc_id)\n",
    "                list_of_term_ids.append(term_id)\n",
    "            else:\n",
    "                #list_of_term_ids.append(term_id)\n",
    "                #tupleX = (doc_id, str(term_id))\n",
    "                #list_of_doc_and_terms.append(tupleX[1])\n",
    "                all_collection_of_words.append(current_word)\n",
    "                all_collection_of_word_ids.append((term_id))\n",
    "                list_of_docs.append(doc_id)\n",
    "                list_of_term_ids.append(term_id)\n",
    "            \n",
    "        #list_of_docs.append(doc_id)\n",
    "        docid_write_to_file(document_output_file_name,doc_id,files[i])\n",
    "        # sort on the basis of term_ids\n",
    "    #list_of_doc_and_terms.sort(key = operator.itemgetter(1))\n",
    "    \n",
    "    list_of_doc_and_terms = list(zip(list_of_docs, list_of_term_ids))\n",
    "    list_of_doc_and_terms.sort(key = operator.itemgetter(1))\n",
    "#     print((list_of_doc_and_terms))\n",
    "\n",
    "    dict_for_termid_and_postings = create_postings_for_unhashed_ii(list_of_doc_and_terms)\n",
    "    termid_write_to_file(term_output_file_name, all_collection_of_words, all_collection_of_word_ids)\n",
    "    \n",
    "    term_index_write_to_file(term_index_dot_txt, dict_for_termid_and_postings)\n",
    "    \n",
    "    print(\"Happy Ending\")\n",
    "    print(len(all_collection_of_words))\n",
    "#    print(dict_for_termid_and_postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create hashed inverted index execute following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Ending\n",
      "8781\n"
     ]
    }
   ],
   "source": [
    "create_unhashed_inverted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
